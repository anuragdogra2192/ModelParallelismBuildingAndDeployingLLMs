{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview of the Class Environment\n",
    "\n",
    "Before we start looking at how to deploy large models, we should revisit the setup of the lab environment. In this section, we will experiment with tools for resource monitoring. The hardware used in this class may vary between sessions, so the number of GPUs, their memory capacity as well as their interconnect might vary from class to class. The results currently listed are based on 4 A100s with 80GB of memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goals of this notebook are to: \n",
    "* Revisit the hardware configuration at our disposal. \n",
    "* Use key nvidia-smi commands to monitor NVIDIA GPUs. \n",
    "* Run test scripts to measure the peer-to-peer communication performance of the NVLINK bus which will be essential for model parallel communication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hardware Overview \n",
    "\n",
    "Let us have a look at the key components of the hardware system at our disposal. As discussed earlier, the configuration of the system does vary between deliveries so you might see different results than some of your classmates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CPU\n",
    "\n",
    "Let us start by inspecting the type of CPU used as well as the number of cores at our disposal: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:            x86_64\n",
      "  CPU op-mode(s):        32-bit, 64-bit\n",
      "  Address sizes:         48 bits physical, 48 bits virtual\n",
      "  Byte Order:            Little Endian\n",
      "CPU(s):                  96\n",
      "  On-line CPU(s) list:   0-95\n",
      "Vendor ID:               AuthenticAMD\n",
      "  Model name:            AMD EPYC 7V13 64-Core Processor\n",
      "    CPU family:          25\n",
      "    Model:               1\n",
      "    Thread(s) per core:  1\n",
      "    Core(s) per socket:  48\n",
      "    Socket(s):           2\n",
      "    Stepping:            1\n",
      "    BogoMIPS:            4890.87\n",
      "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
      "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\n",
      "                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\n",
      "                         od nopl tsc_reliable nonstop_tsc cpuid extd_apicid aper\n",
      "                         fmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 \n",
      "                         movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_\n",
      "                         lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowpre\n",
      "                         fetch osvw topoext perfctr_core invpcid_single vmmcall \n",
      "                         fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx sm\n",
      "                         ap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsave\n",
      "                         s clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdp\n",
      "                         id fsrm\n",
      "Virtualization features: \n",
      "  Hypervisor vendor:     Microsoft\n",
      "  Virtualization type:   full\n",
      "Caches (sum of all):     \n",
      "  L1d:                   3 MiB (96 instances)\n",
      "  L1i:                   3 MiB (96 instances)\n",
      "  L2:                    48 MiB (96 instances)\n",
      "  L3:                    384 MiB (12 instances)\n",
      "NUMA:                    \n",
      "  NUMA node(s):          4\n",
      "  NUMA node0 CPU(s):     0-23\n",
      "  NUMA node1 CPU(s):     24-47\n",
      "  NUMA node2 CPU(s):     48-71\n",
      "  NUMA node3 CPU(s):     72-95\n",
      "Vulnerabilities:         \n",
      "  Gather data sampling:  Not affected\n",
      "  Itlb multihit:         Not affected\n",
      "  L1tf:                  Not affected\n",
      "  Mds:                   Not affected\n",
      "  Meltdown:              Not affected\n",
      "  Mmio stale data:       Not affected\n",
      "  Retbleed:              Not affected\n",
      "  Spec rstack overflow:  Mitigation; safe RET, no microcode\n",
      "  Spec store bypass:     Vulnerable\n",
      "  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer\n",
      "                          sanitization\n",
      "  Spectre v2:            Mitigation; Retpolines, STIBP disabled, RSB filling, PB\n",
      "                         RSB-eIBRS Not affected\n",
      "  Srbds:                 Not affected\n",
      "  Tsx async abort:       Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cores\t: 48\n"
     ]
    }
   ],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will host the NLP model alone and make a limited number of concurrent requests for that model. This will mainly use the GPU, but not much of the GPU. In the case of most production systems, one would deploy not just the model in isolation. An end-to-end production pipeline would include data pre and post processing steps. Production systems also experience much higher traffic, creating higher demand on the CPU, which needs to handle the processing of incoming requests (e.g. Triton Execution overheads). Therefore, maintaining the correct ratio between CPU and GPU resource is critical. Please reach out to your local NVIDIA team for a more detailed conversation about the design of inference systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GPU\n",
    "\n",
    "As before let us list the number and type of available GPUs. As the class environments will vary, there may be anywhere from four to eight Volta V100 GPUs with either 16G or 32G of onboard high bandwidth memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  9 11:11:16 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              52W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              58W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check available GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interconnect Topology\n",
    "\n",
    "As discussed in lab 1, the GPUs we are using today are interconnected using [NVIDIA NVLink technology](https://www.nvidia.com/en-us/data-center/nvlink/). It allows workloads that have high bandwidth and low latency communication to overcome the limitations of PCIe technology. Inference of deep neural networks is in principle an \"embracingly parallel\" workload which is enhanced by the connectivity between the GPUs. Large models, that do not fit into a single GPU (such as recommender systems) create a high requirement for both required bandwidth and also latency. For such models, parallel deployments with NVLINK is a key technology enabling real time execution. Let us inspect the interconnect between the GPUs. Please use the `nvidia-smi topo --matrix` command below to check the topology of our NVLINK interconnect. Depending on the setup of the class we should see all 4 GPUs connected to each other (like in the example output listed below) or 8 GPUS interconnected (in this situation not all GPUs have direct NVLINK interconnect). \n",
    "\n",
    "```\n",
    "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      NV12    SYS     SYS     0-23            N/A\n",
    "GPU1    NV12     X      SYS     SYS     24-47           N/A\n",
    "GPU2    SYS     SYS      X      NV12    48-71           N/A\n",
    "GPU3    SYS     SYS     NV12     X      72-95           N/A\n",
    "\n",
    "Where X= Self and NV# = Connection traversing a bonded set of # NVLinks\n",
    "```\n",
    "\n",
    "On Ampere and Hopper based NVLINK enabled systems, one can find also NVSWITCH overcoming the above-mentioned limitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNV12\tSYS\tSYS\t0-23\t0\t\tN/A\n",
      "GPU1\tNV12\t X \tSYS\tSYS\t24-47\t1\t\tN/A\n",
      "GPU2\tSYS\tSYS\t X \tNV12\t48-71\t2\t\tN/A\n",
      "GPU3\tSYS\tSYS\tNV12\t X \t72-95\t3\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "# Check Interconnect Topology \n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check NVLink status and capabilities using `nvidia-smi nvlink --status` command. On a 4 GPU based system there should be an output listing NVLink capabilities of each GPU like the below:\n",
    "```\n",
    "GPU 0: Graphics Device\n",
    "\t Link 0: 25 GB/s\n",
    "\t Link 1: 25 GB/s\n",
    "\t Link 2: 25 GB/s\n",
    "\t Link 3: 25 GB/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-c7f411c3-fb3c-4e51-f10c-5364df765b96)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-dd612ab8-8ae9-a6ce-1aaf-6a8e155808ac)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 2: NVIDIA A100 80GB PCIe (UUID: GPU-c8b4ce3d-6745-6733-b2c2-9d317e553bc9)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 3: NVIDIA A100 80GB PCIe (UUID: GPU-2855137f-2eee-7502-f196-8d36a6a11db6)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n"
     ]
    }
   ],
   "source": [
    "# Check nvlink status\n",
    "!nvidia-smi nvlink --status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Connectivity\n",
    "\n",
    "Let's make an empirical measurement of the bandwidth and latency that we are achieving in our environment. NVIDIA provides an example application called  **p2pBandwidthLatencyTest** that demonstrates CUDA Peer-To-Peer (P2P) data transfers between pairs of GPUs by computing bandwidth and latency while enabling and disabling NVLink connections. This tool is part of the code samples for CUDA Developers [cuda-samples](https://github.com/NVIDIA/cuda-samples.git). It can be downloaded using the following command, but it was pre-downloaded for the purpose of this class: \n",
    "\n",
    "`git clone --depth 1 --branch v11.2 https://github.com/NVIDIA/cuda-samples.git` \n",
    "\n",
    "To test the bandwidth and latency, please execute the below commands. Please pay particular attention to comparison of results where \"P2P=Disabled\" vs \"P2P=Enabled\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 770 ./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
      "Device: 0, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:1\n",
      "Device: 1, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:2\n",
      "Device: 2, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:3\n",
      "Device: 3, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:4\n",
      "Device=0 CAN Access Peer Device=1\n",
      "Device=0 CANNOT Access Peer Device=2\n",
      "Device=0 CANNOT Access Peer Device=3\n",
      "Device=1 CAN Access Peer Device=0\n",
      "Device=1 CANNOT Access Peer Device=2\n",
      "Device=1 CANNOT Access Peer Device=3\n",
      "Device=2 CANNOT Access Peer Device=0\n",
      "Device=2 CANNOT Access Peer Device=1\n",
      "Device=2 CAN Access Peer Device=3\n",
      "Device=3 CANNOT Access Peer Device=0\n",
      "Device=3 CANNOT Access Peer Device=1\n",
      "Device=3 CAN Access Peer Device=2\n",
      "\n",
      "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
      "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
      "\n",
      "P2P Connectivity Matrix\n",
      "     D\\D     0     1     2     3\n",
      "     0\t     1     1     0     0\n",
      "     1\t     1     1     0     0\n",
      "     2\t     0     0     1     1\n",
      "     3\t     0     0     1     1\n",
      "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1506.75  21.68  21.53  21.29 \n",
      "     1  21.90 1508.20  21.54  21.29 \n",
      "     2  21.80  21.51 1508.20  21.08 \n",
      "     3  21.73  21.62  21.31 1509.66 \n",
      "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1511.12 275.07  21.51  21.29 \n",
      "     1 275.69 1522.90  21.55  21.31 \n",
      "     2  21.55  21.78 1508.20 275.41 \n",
      "     3  21.57  21.64 275.32 1524.39 \n",
      "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1531.11  29.47  27.04  26.73 \n",
      "     1  29.44 1531.86  27.23  26.84 \n",
      "     2  29.49  29.58 1531.11  25.56 \n",
      "     3  29.30  29.34  25.43 1530.36 \n",
      "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1529.61 517.49  27.08  26.91 \n",
      "     1 516.03 1527.37  27.15  26.84 \n",
      "     2  29.54  29.60 1531.86 517.04 \n",
      "     3  29.36  29.34 516.36 1531.11 \n",
      "P2P=Disabled Latency Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.54  24.29  18.69  18.78 \n",
      "     1  12.21   2.64  17.55  18.76 \n",
      "     2  19.99  16.60   2.70  11.52 \n",
      "     3  11.78  12.50  11.67   2.17 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.03   5.75   7.13   7.18 \n",
      "     1   5.71   2.11   6.56   7.13 \n",
      "     2   6.22   6.28   2.29   7.01 \n",
      "     3   6.93   6.28   7.09   2.24 \n",
      "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.53   2.24  18.91  18.71 \n",
      "     1   2.28   2.64  18.73  17.98 \n",
      "     2  12.72  12.59   2.69   2.30 \n",
      "     3  12.37  11.34   2.31   2.17 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.04   1.86   7.59   6.60 \n",
      "     1   1.76   2.03   6.70   7.28 \n",
      "     2   6.31   6.32   2.28   1.93 \n",
      "     3   6.99   6.95   2.09   2.34 \n",
      "\n",
      "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n"
     ]
    }
   ],
   "source": [
    "!./cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Now that we have reviewed the information about the lab environment, let's begin the model deployment. <br> \n",
    "\n",
    "Please proceed to the following notebook to start the next section of the lab: [Inference of the LLama 13B model with HuggingFace.](02_HFRunInferenceOfTheLLama.ipynb) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
