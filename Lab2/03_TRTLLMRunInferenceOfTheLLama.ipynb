{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Optimizing inference with NVIDIA TensorRT-LLM library \n",
    "\n",
    "In this lab, we are going to look at the NVIDIA TensorRT-LLM library and how it optimizes execution of large language models. We will use it to deploy Llama 13B initially using just a single GPU but afterwards taking advantage of its Tensor and Pipeline parallelism capabilities on multiple GPUs.  \n",
    "\n",
    "We will conclude this notebook by comparing the latency between our baseline implementation using the Transformers library and the TensorRT-LLM Tensor and Pipeline parallel deployments. In the next notebook, we will look at how to serve our TensorRT-LLM optimized model to customers/users using Triton Inference Server. \n",
    "\n",
    "To summarize, in this notebook we will: \n",
    "* Review the features of NVIDIA TensorRT-LLM library. \n",
    "* Learn how to build the development environment including building TensorRT-LLM library. \n",
    "* Learn how to prepare a checkpoint of LLama2 (or other Transformers based model) for inference with TensorRT LLM. \n",
    "* Run inference of the model on a single GPU. \n",
    "* Extend the execution to multiple GPUs using Tensor Parallelism. \n",
    "* Profile the single and multi-GPU pipelines to capture information about throughput and latency. \n",
    "\n",
    "**[3.1 NVIDIA TensorRT-LLM](#3.1)<br>** \n",
    "**[3.2 Overall Inference Pipeline with NVIDIA TensorRT-LLM](#3.2)<br>** \n",
    "**[3.3 Download and Install NVIDIA TensorRT-LLM library](#3.3)<br>** \n",
    "**[3.4 Download LLama2 weights](#3.4)<br>** \n",
    "**[3.5 Compile TensorRT-LLM engines](#3.5)<br>** \n",
    "**[3.6 Run TensorRT-LLM engines](#3.6)<br>** \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.6.1 Inference on 1 GPU ](#3.6.1)<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[3.6.2 Inference on 2 GPUs ](#3.6.2)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 NVIDIA TensorRT-LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "\n",
    "In 2020, OpenAI demonstrated that using a large language model trained in a self-supervised way on large volume of training data can significantly improve the capacity of GPT model ([refer to the paper for more details](https://arxiv.org/abs/2005.14165)). The largest GPT-3 variant, has 175 billion parameters, which consumes about 350 GBs, even when represented in half-precision. Therefore putting such a model on a single GPU is impossible, making multi-GPU or even multi-node deployment a necessity. To solve the challenges of latency and memory footprint, the FasterTransformer library provides high efficiency kernels, optimized for memory usage, and support for model parallelism.</br>\n",
    "[NVIDIA’s TensorRT-LLM (FT)](https://github.com/NVIDIA/TensorRT-LLM) is an open-source library for optimal performance on the latest Large Language Models for inference on NVIDIA GPUs. It consists of the TensorRT deep learning compiler and includes optimized kernels, pre– and post-processing and multi-GPU/multi-node communication primitives steps – largely inspired from the former Faster Transformer library for groundbreaking performance on NVIDIA GPUs.\n",
    "It enables you to experiment with new LLMs, with peak performance and quick customization capabilities, without requiring a deep knowledge of C++ or NVIDIA CUDA, as it is offering a convenient Python API.</br>\n",
    "TensorRT-LLM comes with several popular models pre-defined. They can easily be modified and extended to fit custom needs. See for a list of supported [models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm/models).\n",
    "\n",
    "It also comes with a wide range of other features including:</br> \n",
    "* Number of attention layers and caching methods:\n",
    "    * Multi-head Attention(MHA)\n",
    "    * Multi-query Attention (MQA)\n",
    "    * Group-query Attention(GQA)\n",
    "    * Paged KV Cache for the Attention\n",
    "* Support for a range of data types and quantization methods:\n",
    "    * INT4/INT8 Weight-Only Quantization (W4A16 & W8A16)\n",
    "    * FP8\n",
    "    * SmoothQuant\n",
    "    * GPTQ, AWQ\n",
    "* Advanced feature: \n",
    "    * In-flight Batching\n",
    "    * Tensor Parallelism\n",
    "    * Pipeline Parallelism\n",
    "    * Greedy-search\n",
    "    * Beam-search\n",
    "    * RoPE\n",
    "\n",
    "This section of the notebook discusses how TensorRT-LLM can be used for optimization of the LLama2 model. It explains the optimization workflow for both single and multi GPU deployments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor and Pipeline Parallelism \n",
    "\n",
    "Under the hood, TensorRT-LLM relies on MPI and NVIDIA NCCL to enable inter/intra node communication. Using this software stack, anyone can run huge Transformers in Tensor-Parallelism mode on multiple GPUs to reduce computational latency. At the same time, tensor parallelism and pipeline parallelism can be combined to execute large models with billions and trillions of parameters (which amount to terabytes of weights) in Multi-GPU and Multi-Node environments. \n",
    "\n",
    "We have discussed the techniques below in the lecture but let us revisit them before diving into the implementation detail: \n",
    "- Data Parallelism (DP) - is a technique used during the training process. Every GPU receives the same copy of the model but different data to process. The GPUs execute the forward pass in parallel and exchange the gradients during the backward pass, allowing all the devices to make a synchronized weights update based on the average of the accumulated gradients. \n",
    "- Tensor Parallelism (TP) - is a technique used both during training and inference. Instead of splitting the data across multiple GPUs, selected layers of the model are distributed. If using Tensor Parallelism across 8 GPUs each layer affected/its tensor is split into 8 segments, each processed on a separate GPU in parallel. The results are gathered at the end of the step. \n",
    "- Pipeline Parallelism (PP) - similarly, this is a technique used both in training and inference. Here, individual layers are not being split into pieces, instead they are sequentially distributed across multiple GPUs. E.g. if training a 10 layer deep neural network across 2 GPUs, the first five layers would be deployed on the first GPU and the rest on the second GPU. Each GPU is processing data sequentially and the second GPU needs to wait for results from the first GPU. \n",
    "\n",
    "The diagram below demonstrates the difference between Tensor and Pipeline parallelism. \n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"./images/image3.png\" style=\"width: 1000px;\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations in TensorRT-LLM library \n",
    "\n",
    "  \n",
    "\n",
    "TensorRT-LLM allows us to speed up the inference pipeline achieving lower latency and higher throughput compared to the common deep learning frameworks. Below are the key optimization techniques that allow TensorRT-LLM to achieve its performance: \n",
    "1. <b>Layer Fusion</b></br> \n",
    "During the model pre-processing stage, certain layers can be combined to form individual execution kernels. This allows for considerable reduction in GPU memory bandwidth increasing mathematical density of our model, thus accelerating computation at the inference stage. For example, all operations in the multi-head attention block can be combined into a single kernel. \n",
    "2. <b>Autoregressive models: Keys/Values caching. </b></br> \n",
    "In the generation phase, a common optimization is to provide the MultiHeadAttention kernel with a cache containing the values of the past K and V elements that have already been computed. That cache is known as the KV cache. The diagram below illustrates the process. TensorRT-LLM uses that technique to accelerate its generation phase. In TensorRT-LLM, there is one KV cache per Transformer layer, which means that there are as many KV caches as layers in a model. The current version of TensorRT-LLM supports two different types of KV caches: contiguous and paged KV caches.<br/> \n",
    "<div style=\"text-align:center\"> \n",
    "<img src=\"./images/KV_caching v2.PNG\" style=\"width: 50%;position:relative;\"><br/> \n",
    "<em>Keys/Values caching</em> \n",
    "</div> \n",
    "<br/><br/> \n",
    "3. <b>Usage of MPI and NCCL to enable inter/intra node communication and support model parallelism. </b></br> \n",
    "TensorRT-LLM adds the support for systems with multiple GPUs and nodes. It is enabled using TensorRT plugins that wrap communication primitives from the NCCL library as well as a custom plugin that optimize the All-Reduce primitive in the presence of All-to-all connections between GPUs (through NVSwitch in DGX systems).</br> \n",
    "Tensor Parallelism usually leads to more balanced executions but requires more memory bandwidth between the GPUs. Pipeline Parallelism reduces the need for high-bandwidth communication but may incur load-balancing issues and may be less efficient in terms of GPU utilization.</br>\n",
    "4. <b>Reduced precision inference</b></br> \n",
    "TensorRT-LLM has kernels that support inference using low-precision input data in fp32, fp16, bf6, fp8, int8 and int4. All these regimes allow acceleration due to the reduction in data transfer and required memory. Int8 and fp16 computations can be hardware accelerated using TensorCores (available on all GPU architectures starting from Volta) and fp8 using Transformer Engines (starting from Hopper)</br>\n",
    "5. <b>Other optimizations include:</b></br> \n",
    "TensorRT-LLM supports in-flight batching of requests (also known as continuous batching or iteration-level batching) for higher serving throughput. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Overall Inference Pipeline with NVIDIA TensorRT-LLM\n",
    "The diagram listed below lists all the steps involved in using the TensorRT-LLM library to deploy large models to production. In the next section, we will go through them one at a time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/TRTLLM_pipeline.png\" style=\"width: 30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Download and install NVIDIA TensorRT-LLM library\n",
    "Starting with Triton 23.10 release, Triton includes a container with the TensorRT-LLM Backend and the Python Backend. This container should have everything needed to run a TensorRT-LLM model. You can find this container [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver).\n",
    "\n",
    "In this lab, we already git cloned the backend repository for you by using the commands: </br>\n",
    "```\n",
    "git clone -b release/0.6.1 https://github.com/triton-inference-server/TensorRT-LLM_backend.git\n",
    "cd tensorrtllm_backend \n",
    "```\n",
    "We then fetched the TensorRT-LLM library as a submodule:</br>\n",
    "```\n",
    "git submodule update --init --recursive\n",
    "git lfs install\n",
    "git lfs pull\n",
    "```\n",
    "And install properly the TensorRT-LLM library in the Triton container: </br>\n",
    "```\n",
    "pip install git+https://github.com/NVIDIA/TensorRT-LLM.git\n",
    "mkdir /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/\n",
    "cp /opt/tritonserver/backends/tensorrtllm/* /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Download LLama Weights\n",
    "We already downloaded the model weights from the [Meta website](https://llama.meta.com/). Please view the [license agreement](https://ai.meta.com/llama/license/). If you would like to use a Llama model outside of this course, please [register with Meta](https://llama.meta.com/llama-downloads).\n",
    "\n",
    "Once downloaded, we converted the weights into Hugging Face format using the `src/transformers/models/llama/convert_llama_weights_to_hf.py` python script from Transformers Library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>/dli/task/weights</b> folder's content should look similar to the following:\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"./images/llama_weights_folder_new.png\" style=\"width: 50%\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Build TensorRT-LLM engines\n",
    "### Build on 1 GPU\n",
    "\n",
    "In this section, we are going to build TensorRT-LLM engines from the Hugging Face Llama weights, first on 1 GPU, and then on 4 GPUs using model parallelism.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task/tensorrtllm_backend/tensorrt_llm/examples/llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.14.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.14.5)\n",
      "Requirement already satisfied: rouge_score~=0.1.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: sentencepiece~=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets==2.14.5->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (3.9.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (0.22.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score~=0.1.2->-r requirements.txt (line 2)) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score~=0.1.2->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5->-r requirements.txt (line 1)) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5->-r requirements.txt (line 1)) (2023.7.22)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score~=0.1.2->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5->-r requirements.txt (line 1)) (2024.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (4.25.2)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.2\n",
      "    Uninstalling protobuf-4.25.2:\n",
      "      Successfully uninstalled protobuf-4.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorrt-llm 0.5.0 requires accelerate==0.20.3, but you have accelerate 0.26.1 which is incompatible.\n",
      "tensorrt-llm 0.5.0 requires transformers==4.31.0, but you have transformers 4.37.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.26.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd /dli/task/tensorrtllm_backend/tensorrt_llm/examples/llama\n",
    "%pip install -r requirements.txt\n",
    "%pip install --upgrade protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the output directory where to store the compiled engine: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dli/task\n"
     ]
    }
   ],
   "source": [
    "%cd /dli/task\n",
    "trt_engine_1gpu=\"/dli/task/trt-engines/llama_13b/fp16/1-gpu\"\n",
    "!mkdir -p $trt_engine_1gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Build command takes some parameters, each of them impacting performance of the engine at different level (Memory used for KV Caching, Batching policy, Quantization, ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/09/2024-13:30:51] [TRT-LLM] [I] Serially build TensorRT engines.\n",
      "[05/09/2024-13:30:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 125, GPU 27412 (MiB)\n",
      "[05/09/2024-13:30:53] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1973, GPU +350, now: CPU 2234, GPU 27762 (MiB)\n",
      "[05/09/2024-13:30:53] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[05/09/2024-13:30:57] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 13.11it/s]\n",
      "[05/09/2024-13:31:01] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:04\n",
      "[05/09/2024-13:31:01] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:31:03] [TRT-LLM] [I] Weights loaded. Total time: 00:00:01\n",
      "[05/09/2024-13:31:04] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:31:04] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:31:04] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:31:04] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp1_rank0.engine\n",
      "[05/09/2024-13:31:04] [TRT] [W] Unused Input: position_ids\n",
      "[05/09/2024-13:31:04] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[05/09/2024-13:31:04] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 27117, GPU 27856 (MiB)\n",
      "[05/09/2024-13:31:04] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 27119, GPU 27866 (MiB)\n",
      "[05/09/2024-13:31:04] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:31:15] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:15] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:15] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:15] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:16] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:16] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:16] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:16] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:16] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:16] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:20] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:20] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:21] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:21] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 0 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000018.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 1 due to insufficient memory on requested size of 82944 detected for tactic 0x0000000000000019.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 2 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001a.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 3 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001b.\n",
      "[05/09/2024-13:31:22] [TRT] [W] Tactic Device request: 82944MB Available: 81050MB. Device memory is insufficient to use tactic.\n",
      "[05/09/2024-13:31:22] [TRT] [W] UNSUPPORTED_STATESkipping tactic 4 due to insufficient memory on requested size of 82944 detected for tactic 0x000000000000001f.\n",
      "[05/09/2024-13:31:24] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:31:24] [TRT] [I] Detected 49 inputs and 1 output network tensors.\n",
      "[05/09/2024-13:31:34] [TRT] [I] Total Host Persistent Memory: 112416\n",
      "[05/09/2024-13:31:34] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:31:34] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:31:34] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 691 steps to complete.\n",
      "[05/09/2024-13:31:34] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 19.742ms to assign 11 blocks to 691 nodes requiring 1610615808 bytes.\n",
      "[05/09/2024-13:31:34] [TRT] [I] Total Activation Memory: 1610615808\n",
      "[05/09/2024-13:31:34] [TRT] [I] Total Weights Memory: 26031728664\n",
      "[05/09/2024-13:31:34] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 27496, GPU 52706 (MiB)\n",
      "[05/09/2024-13:31:34] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 27496, GPU 52716 (MiB)\n",
      "[05/09/2024-13:31:34] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 30720 MiB\n",
      "[05/09/2024-13:31:34] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +24826, now: CPU 0, GPU 24826 (MiB)\n",
      "[05/09/2024-13:31:38] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 58677 MiB\n",
      "[05/09/2024-13:31:38] [TRT-LLM] [I] Total time of building llama_float16_tp1_rank0.engine: 00:00:33\n",
      "[05/09/2024-13:31:38] [TRT-LLM] [I] Config saved to /dli/task/trt-engines/llama_13b/fp16/1-gpu/config.json.\n",
      "[05/09/2024-13:31:38] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/1-gpu/llama_float16_tp1_rank0.engine...\n",
      "[05/09/2024-13:31:54] [TRT-LLM] [I] Engine serialized. Total time: 00:00:16\n",
      "[05/09/2024-13:31:55] [TRT-LLM] [I] Timing cache serialized to /dli/task/trt-engines/llama_13b/fp16/1-gpu/model.cache\n",
      "[05/09/2024-13:31:55] [TRT-LLM] [I] Total time of building all 1 engines: 00:01:03\n"
     ]
    }
   ],
   "source": [
    "hf_weights_dir = \"/dli/task/weights\"\n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py  \\\n",
    "                --model_dir $hf_weights_dir \\\n",
    "                --dtype float16 \\\n",
    "                --use_gpt_attention_plugin float16  \\\n",
    "                --use_inflight_batching \\\n",
    "                --paged_kv_cache \\\n",
    "                --remove_input_padding \\\n",
    "                --use_gemm_plugin float16  \\\n",
    "                --output_dir $trt_engine_1gpu  \\\n",
    "                --max_input_len 2048 --max_output_len 512 \\\n",
    "                --use_rmsnorm_plugin float16  \\\n",
    "                --enable_context_fmha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  llama_float16_tp1_rank0.engine  model.cache\n"
     ]
    }
   ],
   "source": [
    "# Check your output ! You should have .engine in the folder now\n",
    "!ls $trt_engine_1gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build on 4 GPUs\n",
    "Large Language Models can be huge and the GPU RAM can be a limitation.\n",
    "Pipeline and Tensor Parallelism (PP and TP) are efficient ways to workaround the memory limitation on a single GPU as they split the model into parts at training and inference time and distribute them among multiple GPUs.</br>\n",
    "Let's see it in action on Llama-13B.\n",
    "The world size will represent the number of parts you have.\n",
    "For example, using PP=2 and TP=2, the world size is equal to 4.\n",
    "\n",
    "Prepare the output directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engine_4gpus= \"/dli/task/trt-engines/llama_13b/fp16/4-gpus\"\n",
    "!mkdir -p $trt_engine_4gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your engine using Tp_size and PP_size flags\n",
    "World_size must be equal to  Tp_size * PP_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/09/2024-13:32:22] [TRT-LLM] [I] Serially build TensorRT engines.\n",
      "[05/09/2024-13:32:22] [TRT] [I] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 125, GPU 27412 (MiB)\n",
      "[05/09/2024-13:32:24] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1973, GPU +350, now: CPU 2234, GPU 27762 (MiB)\n",
      "[05/09/2024-13:32:24] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[05/09/2024-13:32:25] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 13.24it/s]\n",
      "[05/09/2024-13:32:26] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:00\n",
      "[05/09/2024-13:32:26] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:32:28] [TRT-LLM] [I] Weights loaded. Total time: 00:00:02\n",
      "[05/09/2024-13:32:28] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:32:28] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:32:28] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:32:28] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_pp2_rank0.engine\n",
      "[05/09/2024-13:32:28] [TRT] [W] Unused Input: position_ids\n",
      "[05/09/2024-13:32:28] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[05/09/2024-13:32:28] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 11352, GPU 27856 (MiB)\n",
      "[05/09/2024-13:32:28] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 11354, GPU 27866 (MiB)\n",
      "[05/09/2024-13:32:28] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:32:45] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:32:45] [TRT] [I] Detected 28 inputs and 1 output network tensors.\n",
      "[05/09/2024-13:32:48] [TRT] [I] Total Host Persistent Memory: 60624\n",
      "[05/09/2024-13:32:48] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:32:48] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:32:48] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 387 steps to complete.\n",
      "[05/09/2024-13:32:48] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 10.6637ms to assign 11 blocks to 387 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:32:48] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:32:48] [TRT] [I] Total Weights Memory: 6671974424\n",
      "[05/09/2024-13:32:48] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 11729, GPU 34244 (MiB)\n",
      "[05/09/2024-13:32:48] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 11729, GPU 34254 (MiB)\n",
      "[05/09/2024-13:32:49] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:32:49] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +6363, now: CPU 0, GPU 6363 (MiB)\n",
      "[05/09/2024-13:32:49] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 42975 MiB\n",
      "[05/09/2024-13:32:49] [TRT-LLM] [I] Total time of building llama_float16_tp2_pp2_rank0.engine: 00:00:21\n",
      "[05/09/2024-13:32:49] [TRT-LLM] [I] Config saved to /dli/task/trt-engines/llama_13b/fp16/4-gpus/config.json.\n",
      "[05/09/2024-13:32:49] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/4-gpus/llama_float16_tp2_pp2_rank0.engine...\n",
      "[05/09/2024-13:32:53] [TRT-LLM] [I] Engine serialized. Total time: 00:00:03\n",
      "[05/09/2024-13:32:54] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 12.98it/s]\n",
      "[05/09/2024-13:32:55] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:00\n",
      "[05/09/2024-13:32:55] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:32:57] [TRT-LLM] [I] Weights loaded. Total time: 00:00:02\n",
      "[05/09/2024-13:32:57] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:32:57] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:32:57] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:32:57] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_pp2_rank1.engine\n",
      "[05/09/2024-13:32:57] [TRT] [W] Unused Input: position_ids\n",
      "[05/09/2024-13:32:57] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[05/09/2024-13:32:57] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 26843, GPU 27864 (MiB)\n",
      "[05/09/2024-13:32:57] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 26843, GPU 27872 (MiB)\n",
      "[05/09/2024-13:32:57] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:32:58] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:32:58] [TRT] [I] Detected 28 inputs and 1 output network tensors.\n",
      "[05/09/2024-13:33:00] [TRT] [I] Total Host Persistent Memory: 60624\n",
      "[05/09/2024-13:33:00] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:33:00] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:33:00] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 387 steps to complete.\n",
      "[05/09/2024-13:33:00] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 10.3982ms to assign 11 blocks to 387 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:33:00] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:33:00] [TRT] [I] Total Weights Memory: 6671974424\n",
      "[05/09/2024-13:33:00] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 27158, GPU 34244 (MiB)\n",
      "[05/09/2024-13:33:00] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 27158, GPU 34252 (MiB)\n",
      "[05/09/2024-13:33:00] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:33:00] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +6363, now: CPU 0, GPU 6363 (MiB)\n",
      "[05/09/2024-13:33:01] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 58547 MiB\n",
      "[05/09/2024-13:33:01] [TRT-LLM] [I] Total time of building llama_float16_tp2_pp2_rank1.engine: 00:00:03\n",
      "[05/09/2024-13:33:01] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/4-gpus/llama_float16_tp2_pp2_rank1.engine...\n",
      "[05/09/2024-13:33:05] [TRT-LLM] [I] Engine serialized. Total time: 00:00:03\n",
      "[05/09/2024-13:33:06] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 14.18it/s]\n",
      "[05/09/2024-13:33:06] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:00\n",
      "[05/09/2024-13:33:06] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:33:08] [TRT-LLM] [I] Weights loaded. Total time: 00:00:01\n",
      "[05/09/2024-13:33:08] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:33:08] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:33:08] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:33:08] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_pp2_rank2.engine\n",
      "[05/09/2024-13:33:08] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 35908, GPU 27864 (MiB)\n",
      "[05/09/2024-13:33:08] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 35908, GPU 27872 (MiB)\n",
      "[05/09/2024-13:33:08] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:33:09] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:33:09] [TRT] [I] Detected 28 inputs and 3 output network tensors.\n",
      "[05/09/2024-13:33:11] [TRT] [I] Total Host Persistent Memory: 61088\n",
      "[05/09/2024-13:33:11] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:33:11] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:33:11] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 392 steps to complete.\n",
      "[05/09/2024-13:33:11] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 13.9406ms to assign 11 blocks to 392 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:33:11] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:33:11] [TRT] [I] Total Weights Memory: 6508144648\n",
      "[05/09/2024-13:33:11] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 35910, GPU 34088 (MiB)\n",
      "[05/09/2024-13:33:11] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 35910, GPU 34096 (MiB)\n",
      "[05/09/2024-13:33:11] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:33:11] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +6207, now: CPU 0, GPU 6207 (MiB)\n",
      "[05/09/2024-13:33:12] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 67676 MiB\n",
      "[05/09/2024-13:33:12] [TRT-LLM] [I] Total time of building llama_float16_tp2_pp2_rank2.engine: 00:00:03\n",
      "[05/09/2024-13:33:12] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/4-gpus/llama_float16_tp2_pp2_rank2.engine...\n",
      "[05/09/2024-13:33:15] [TRT-LLM] [I] Engine serialized. Total time: 00:00:03\n",
      "[05/09/2024-13:33:16] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 14.14it/s]\n",
      "[05/09/2024-13:33:17] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:00\n",
      "[05/09/2024-13:33:17] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:33:19] [TRT-LLM] [I] Weights loaded. Total time: 00:00:02\n",
      "[05/09/2024-13:33:19] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:33:19] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:33:19] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:33:19] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_pp2_rank3.engine\n",
      "[05/09/2024-13:33:19] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 44816, GPU 27864 (MiB)\n",
      "[05/09/2024-13:33:19] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 44816, GPU 27872 (MiB)\n",
      "[05/09/2024-13:33:19] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:33:19] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:33:19] [TRT] [I] Detected 28 inputs and 3 output network tensors.\n",
      "[05/09/2024-13:33:21] [TRT] [I] Total Host Persistent Memory: 61088\n",
      "[05/09/2024-13:33:21] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:33:21] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:33:21] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 392 steps to complete.\n",
      "[05/09/2024-13:33:21] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 12.2779ms to assign 11 blocks to 392 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:33:21] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:33:21] [TRT] [I] Total Weights Memory: 6508144648\n",
      "[05/09/2024-13:33:21] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 44818, GPU 34088 (MiB)\n",
      "[05/09/2024-13:33:21] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 44819, GPU 34096 (MiB)\n",
      "[05/09/2024-13:33:21] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:33:21] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +6207, now: CPU 0, GPU 6207 (MiB)\n",
      "[05/09/2024-13:33:22] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 76596 MiB\n",
      "[05/09/2024-13:33:22] [TRT-LLM] [I] Total time of building llama_float16_tp2_pp2_rank3.engine: 00:00:03\n",
      "[05/09/2024-13:33:22] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/4-gpus/llama_float16_tp2_pp2_rank3.engine...\n",
      "[05/09/2024-13:33:26] [TRT-LLM] [I] Engine serialized. Total time: 00:00:03\n",
      "[05/09/2024-13:33:26] [TRT-LLM] [I] Timing cache serialized to /dli/task/trt-engines/llama_13b/fp16/4-gpus/model.cache\n",
      "[05/09/2024-13:33:26] [TRT-LLM] [I] Total time of building all 4 engines: 00:01:03\n"
     ]
    }
   ],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir $hf_weights_dir \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir $trt_engine_4gpus \\\n",
    "    --world_size 4 \\\n",
    "    --tp_size 2 \\\n",
    "    --pp_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t    llama_float16_tp2_pp2_rank2.engine\n",
      "llama_float16_tp2_pp2_rank0.engine  llama_float16_tp2_pp2_rank3.engine\n",
      "llama_float16_tp2_pp2_rank1.engine  model.cache\n"
     ]
    }
   ],
   "source": [
    "# Check your output ! You should have 4 engines in the folder, one for each rank\n",
    "!ls $trt_engine_4gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Run the TensorRT-LLM engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on 1 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your engine are now ready to run using TensorRT-LLM library! \n",
    "The tokenizer files are stored next to the weights in the Llama folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the float16 engine ...\n",
      "Input: \"How do I count in French ? 1 un\"\n",
      "Output: \"2 deux 3 trois 4 quatre 5 cinq 6 six 7 sept 8 huit 9 neuf 10 dix 11 onze 12 douze 13 treize 14 quatorze 15 quinze 16 seize 17 dix-sept 18 dix-huit 19 dix-neuf 20 vingt 21 vingt et un 22 vingt et deux 23 vingt et trois 24 vingt et quatre 25 vingt et cinq 26 vingt et\"\n"
     ]
    }
   ],
   "source": [
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_1gpu \\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French ? 1 un\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on 4 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mpirun command to launch the Run command on TensorRT-LLM for multi-GPUs execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the float16 engine ...\n",
      "Input: \"How do I count in French? 1 un \"\n",
      "Output: \"2 deux 3 trois 4 quatre 5 cinq 6 six 7 sept 8 huit 9 neuf 10 dix 11 onze 12 douze 13 treize 14 quatorze 15 quinze 16 seize 17 dix-sept 18 dix-huit 19 dix-neuf 20 vingt 21 vingt et un 22 vingt et deux 23 vingt et trois 24 vingt et quatre 25 vingt et cinq 26 vingt et six\"\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_4gpus\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French? 1 un \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Exercice - Build and Run on 2 GPus\n",
    "Let's practice yourself, and try to create a TensorRT-LLM engine on 2 GPU using only Tensor Parallelism.</br>\n",
    "1) Prepare your output directory </br>\n",
    "2) Build the engine </br>\n",
    "3) Run and test your engine </br>\n",
    "\n",
    "Fill the <<<< FIXME >>>> in the cells below. If you are stuck, check the solutions clicking on the ... just under each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare your output directory\n",
    "trt_engine_2gpus= \"/dli/task/trt-engines/llama_13b/fp16/2-gpus\"\n",
    "!mkdir -p $trt_engine_2gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOLUTION \n",
    "# 1) Prepare your output directory\n",
    "trt_engine_2gpus= \"/dli/task/trt-engines/llama_13b/fp16/2-gpus\"\n",
    "!mkdir -p $trt_engine_2gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/09/2024-13:44:16] [TRT-LLM] [I] Serially build TensorRT engines.\n",
      "[05/09/2024-13:44:17] [TRT] [I] [MemUsageChange] Init CUDA: CPU +14, GPU +0, now: CPU 125, GPU 27412 (MiB)\n",
      "[05/09/2024-13:44:18] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1973, GPU +350, now: CPU 2234, GPU 27762 (MiB)\n",
      "[05/09/2024-13:44:18] [TRT-LLM] [W] Invalid timing cache, using freshly created one\n",
      "[05/09/2024-13:44:20] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 13.26it/s]\n",
      "[05/09/2024-13:44:24] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:03\n",
      "[05/09/2024-13:44:24] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:44:26] [TRT-LLM] [I] Weights loaded. Total time: 00:00:02\n",
      "[05/09/2024-13:44:27] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:44:27] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:44:27] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:44:27] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_rank0.engine\n",
      "[05/09/2024-13:44:27] [TRT] [W] Unused Input: position_ids\n",
      "[05/09/2024-13:44:27] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[05/09/2024-13:44:27] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 20418, GPU 27856 (MiB)\n",
      "[05/09/2024-13:44:27] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 20419, GPU 27866 (MiB)\n",
      "[05/09/2024-13:44:27] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:44:44] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:44:44] [TRT] [I] Detected 49 inputs and 3 output network tensors.\n",
      "[05/09/2024-13:44:48] [TRT] [I] Total Host Persistent Memory: 121488\n",
      "[05/09/2024-13:44:48] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:44:48] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:44:48] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 772 steps to complete.\n",
      "[05/09/2024-13:44:48] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 25.4625ms to assign 11 blocks to 772 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:44:48] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:44:49] [TRT] [I] Total Weights Memory: 13180119064\n",
      "[05/09/2024-13:44:49] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 20796, GPU 40450 (MiB)\n",
      "[05/09/2024-13:44:49] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 20796, GPU 40460 (MiB)\n",
      "[05/09/2024-13:44:49] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:44:49] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12570, now: CPU 0, GPU 12570 (MiB)\n",
      "[05/09/2024-13:44:51] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 52075 MiB\n",
      "[05/09/2024-13:44:51] [TRT-LLM] [I] Total time of building llama_float16_tp2_rank0.engine: 00:00:23\n",
      "[05/09/2024-13:44:51] [TRT-LLM] [I] Config saved to /dli/task/trt-engines/llama_13b/fp16/2-gpus/config.json.\n",
      "[05/09/2024-13:44:51] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/2-gpus/llama_float16_tp2_rank0.engine...\n",
      "[05/09/2024-13:44:58] [TRT-LLM] [I] Engine serialized. Total time: 00:00:07\n",
      "[05/09/2024-13:45:00] [TRT-LLM] [I] Loading HF LLaMA ... from /dli/task/weights\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:00<00:00, 13.96it/s]\n",
      "[05/09/2024-13:45:00] [TRT-LLM] [I] HF LLaMA loaded. Total time: 00:00:00\n",
      "[05/09/2024-13:45:00] [TRT-LLM] [I] Loading weights from HF LLaMA...\n",
      "[05/09/2024-13:45:03] [TRT-LLM] [I] Weights loaded. Total time: 00:00:03\n",
      "[05/09/2024-13:45:04] [TRT-LLM] [I] Context FMHA Enabled\n",
      "[05/09/2024-13:45:04] [TRT-LLM] [I] Remove Padding Enabled\n",
      "[05/09/2024-13:45:04] [TRT-LLM] [I] Paged KV Cache Enabled\n",
      "[05/09/2024-13:45:04] [TRT-LLM] [I] Build TensorRT engine llama_float16_tp2_rank1.engine\n",
      "[05/09/2024-13:45:04] [TRT] [W] Unused Input: position_ids\n",
      "[05/09/2024-13:45:04] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.\n",
      "[05/09/2024-13:45:04] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 51181, GPU 27864 (MiB)\n",
      "[05/09/2024-13:45:04] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 51181, GPU 27872 (MiB)\n",
      "[05/09/2024-13:45:04] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.\n",
      "[05/09/2024-13:45:05] [TRT] [I] [GraphReduction] The approximate region cut reduction algorithm is called.\n",
      "[05/09/2024-13:45:05] [TRT] [I] Detected 49 inputs and 3 output network tensors.\n",
      "[05/09/2024-13:45:08] [TRT] [I] Total Host Persistent Memory: 121488\n",
      "[05/09/2024-13:45:08] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[05/09/2024-13:45:08] [TRT] [I] Total Scratch Memory: 33620096\n",
      "[05/09/2024-13:45:08] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 772 steps to complete.\n",
      "[05/09/2024-13:45:08] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 25.8278ms to assign 11 blocks to 772 nodes requiring 905972736 bytes.\n",
      "[05/09/2024-13:45:08] [TRT] [I] Total Activation Memory: 905972736\n",
      "[05/09/2024-13:45:09] [TRT] [I] Total Weights Memory: 13180119064\n",
      "[05/09/2024-13:45:09] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 51496, GPU 40450 (MiB)\n",
      "[05/09/2024-13:45:09] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 51497, GPU 40458 (MiB)\n",
      "[05/09/2024-13:45:09] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 625 MiB, GPU 41472 MiB\n",
      "[05/09/2024-13:45:09] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +12570, now: CPU 0, GPU 12570 (MiB)\n",
      "[05/09/2024-13:45:11] [TRT] [I] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 82925 MiB\n",
      "[05/09/2024-13:45:11] [TRT-LLM] [I] Total time of building llama_float16_tp2_rank1.engine: 00:00:06\n",
      "[05/09/2024-13:45:11] [TRT-LLM] [I] Serializing engine to /dli/task/trt-engines/llama_13b/fp16/2-gpus/llama_float16_tp2_rank1.engine...\n",
      "[05/09/2024-13:45:18] [TRT-LLM] [I] Engine serialized. Total time: 00:00:07\n",
      "[05/09/2024-13:45:18] [TRT-LLM] [I] Timing cache serialized to /dli/task/trt-engines/llama_13b/fp16/2-gpus/model.cache\n",
      "[05/09/2024-13:45:18] [TRT-LLM] [I] Total time of building all 2 engines: 00:01:01\n"
     ]
    }
   ],
   "source": [
    "# 2) Build the engine \n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir $hf_weights_dir \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir $trt_engine_2gpus \\\n",
    "    --world_size 2 \\\n",
    "    --tp_size 2 \\\n",
    "    --pp_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION \n",
    "# 2) Build the engine \n",
    "!python tensorrtllm_backend/tensorrt_llm/examples/llama/build.py \\\n",
    "    --model_dir $hf_weights_dir \\\n",
    "    --dtype float16 \\\n",
    "    --use_gpt_attention_plugin float16 \\\n",
    "    --use_gemm_plugin float16 \\\n",
    "    --use_rmsnorm_plugin float16 \\\n",
    "    --use_inflight_batching \\\n",
    "    --remove_input_padding \\\n",
    "    --enable_context_fmha \\\n",
    "    --paged_kv_cache \\\n",
    "    --max_input_len 2048 --max_output_len 512 \\\n",
    "    --output_dir $trt_engine_2gpus \\\n",
    "    --world_size 2 \\\n",
    "    --tp_size 2 \\\n",
    "    --pp_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the float16 engine ...\n",
      "Input: \"How do I count in German? 1 eins \"\n",
      "Output: \"2 zwei 3 drei 4 vier 5 fünf 6 sechs 7 sieben 8 acht 9 neun 10 zehn 11 elf 12 zwölf 13 dreizehn 14 vierzehn 15 fünfzehn 16 sechzehn 17 siebzehn 18 achtzehn 19 neunzehn 20 zwanzig 21 einundzwanzig 22 zweiundzwanzig 23 dreiundzwanzig 24 vierundzwanzig 25 fünfund\"\n"
     ]
    }
   ],
   "source": [
    "# 3) Run and test your engine </br>\n",
    "!mpirun -n 2 --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_2gpus\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir\\\n",
    "    --input_text \"How do I count in German? 1 eins \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# 3) Run and test your engine </br>\n",
    "!mpirun -n 2 --allow-run-as-root python tensorrtllm_backend/tensorrt_llm/examples/llama/run.py \\\n",
    "    --engine_dir=$trt_engine_2gpus\\\n",
    "    --max_output_len 128 \\\n",
    "    --tokenizer_dir $hf_weights_dir \\\n",
    "    --input_text \"How do I count in French? 1 un \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Please proceed on to [Inference of the LLama 13B model with Triton Inference server and TensorRT-LLM as a backend.](04_TRTLLMAndTritonRunRemoteInferenceOfTheLlama.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
