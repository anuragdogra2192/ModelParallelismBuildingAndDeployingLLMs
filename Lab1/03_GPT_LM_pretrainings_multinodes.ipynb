{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Multi-Node Distributed Training Strategies\n",
    "\n",
    "In this notebook, we will learn how to run [NeMo](https://github.com/NVIDIA/NeMo/) GPT pretraing on multiple nodes.\n",
    "\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Run simple multi-node training of NeMo Framework scripts\n",
    "* Run a hybrid multi-node execution with data, tensor and pipeline parallel distributions\n",
    "\n",
    "\n",
    "**[3.1 Multi-Node Training Execution of NeMo GPT Pretraining](#3.1-Multi-Node-Training-Execution-of-NeMo-GPT-Pretraining)<br>**\n",
    "**[3.2 Multi-Node Execution with Data Parallelism](#3.2-Multi-Node-Execution-with-Data-Parallelism)<br>**\n",
    "**[3.3 Inter/Intra Node Communications](#3.3-Inter/Intra-Node-Communications)<br>**\n",
    "**[3.4 Monitoring and Profiling the Training](#3.4-Monitoring-and-Profiling-the-Training)<br>**\n",
    "**[3.5 Increase The Batchsize / GPU](#3.5-Increase-The-Batchsize-/-GPU)<br>**\n",
    "**[3.6 Exercise: Hybrid Distributed Training Strategy](#3.6-Exercise:-Hybrid-Distributed-Training-Strategy)<br>**\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.1 Multi-Node Training Execution of NeMo GPT Pretraining\n",
    "\n",
    "In the previous notebook, we submitted our jobs in an interactive session after allocating 1 node. \n",
    "\n",
    "For multi-node jobs, we need to rely on the SLURM scheduler. By default, multi-node training with NeMo Framework uses the [NVIDIA Collective Communications Library - NCCL](https://developer.nvidia.com/nccl) distributed backend of the [PyTorch distributed launcher](https://pytorch.org/docs/stable/elastic/run.html).\n",
    "\n",
    "For 2-Node execution, we will use `SBATCH` scripting. While Python execution commands and their arguments remain similar to how it was done for single node, we will need additional `SBATCH` arguments for resource allocation. \n",
    "\n",
    "Let's start by executing NeMo GPT pretraining on 2 nodes using only Data Parallelism, meaning that the model is copied on the 4 allocated GPUs, each processing different data batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Multi-Node Execution with Data Parallelism\n",
    "\n",
    "In the previous 2-GPU data parallelism execution, the batch size processed by each GPU was 16 (set by `model.micro_batch_size`) corresponding to a global batch size of 32 (set by `model.global_batch_size`). \n",
    "\n",
    "When using 4 GPUs, we can keep the micro batch size per GPUs being 16 and set the global batch size to 64 (`MICRO_BATCH_SIZE` $ \\times $ 4).\n",
    "\n",
    "Let's have a look at the script before allocating resources and executing it. \n",
    "\n",
    "Notice the `SBATCH` arguments for allocating resources. `#SBATCH --ntasks-per-node` should be set to 2 when using NeMo Framework as we will use both GPUs on every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "#SBATCH --job-name=dli_2nodes\n",
      "#SBATCH --nodes=2\n",
      "#SBATCH --ntasks-per-node=2       \n",
      "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
      "#SBATCH -o /dli/nemo/logs/%j.out\n",
      "#SBATCH -e /dli/nemo/logs/%j.err\n",
      "\n",
      "set -x -e\n",
      "\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "\n",
      "# Distributed training args\n",
      "NNODES=2\n",
      "GPUS_PER_NODE=2\n",
      "TP_SIZE=1\n",
      "PP_SIZE=1 \n",
      "\n",
      "# Distributed training \n",
      "MICRO_BATCH_SIZE=16\n",
      "GLOBAL_BATCH_SIZE=64    # <--- CHANGED HERE\n",
      "\n",
      "# Model architecture \n",
      "NLAYERS=12\n",
      "NHIDDEN=768\n",
      "NHEADS=32\n",
      "SEQ_LEN=1024\n",
      "\n",
      "# Data Paths\n",
      "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
      "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
      "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
      "\n",
      "OUTPUT_PATH=/dli/nemo\n",
      "LOGS_PATH=/dli/nemo/logs\n",
      "NAME=\"2Nodes4GPUS\"       # <--- CHANGED HERE \n",
      "\n",
      "\n",
      "OPTIMIZER_ARGS=\" \\\n",
      "            model.optim.name=fused_adam \\\n",
      "            model.optim.betas=[0.9,0.95] \\\n",
      "            model.optim.lr=6e-5 \\\n",
      "            model.optim.sched.min_lr=6e-6 \\\n",
      "            model.optim.sched.name=CosineAnnealing \\\n",
      "            +model.optim.sched.max_steps=800 \\\n",
      "            model.optim.sched.warmup_steps=80 \\\n",
      "            model.optim.weight_decay=1e-1 \\\n",
      "        \"\n",
      "\n",
      "# ADDED PROFILER BELOW\n",
      "TRAINER_ARGS=\" \\\n",
      "            trainer.gradient_clip_val=1.0 \\\n",
      "            trainer.precision=32 \\\n",
      "            trainer.devices=$GPUS_PER_NODE \\\n",
      "            trainer.num_nodes=$NNODES \\\n",
      "            trainer.max_steps=100 \\\n",
      "            trainer.enable_model_summary=true \\\n",
      "            trainer.log_every_n_steps=10 \\\n",
      "            trainer.val_check_interval=20 \\\n",
      "            trainer.limit_val_batches=10 \\\n",
      "            +trainer.use_profiler=true \\\n",
      "        \"\n",
      "\n",
      "GPT_ARGS=\" \\\n",
      "            model.num_layers=$NLAYERS \\\n",
      "            model.hidden_size=$NHIDDEN \\\n",
      "            model.num_attention_heads=$NHEADS \\\n",
      "            model.encoder_seq_length=$SEQ_LEN \\\n",
      "            model.data.seq_length=$SEQ_LEN \\\n",
      "            model.max_position_embeddings=$SEQ_LEN \\\n",
      "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
      "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
      "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
      "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
      "            model.init_method_std=0.006 \\\n",
      "            $OPTIMIZER_ARGS \\\n",
      "        \"\n",
      "\n",
      "OUTPUT_ARGS=\" \\\n",
      "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
      "            exp_manager.resume_if_exists=false \\\n",
      "            exp_manager.name=$NAME \\\n",
      "        \"\n",
      "\n",
      "PARALLEL_ARGS=\" \\\n",
      "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
      "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
      "        \"\n",
      "\n",
      "export CMD=\" \\\n",
      "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
      "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
      "            --config-name=megatron_gpt_config.yaml \\\n",
      "            $TRAINER_ARGS \\\n",
      "            $PARALLEL_ARGS \\\n",
      "            $GPT_ARGS \\\n",
      "            $OUTPUT_ARGS \\\n",
      "            model.data.data_prefix=$DATA_PATH \\\n",
      "            model.data.data_impl=mmap \\\n",
      "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
      "        \"\n",
      "\n",
      "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt\n"
     ]
    }
   ],
   "source": [
    "# Have a look at NeMo GPT pretraining execution on 2 nodes\n",
    "!cat /dli/code/pretrain_gpt_2Node4GPU.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the SLURM job using `sbatch` [`pretrain_gpt_2Node4GPU.sh`](./code/pretrain_gpt_2Node4GPU.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 12\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                12  slurmpar dli_2nod     root PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the GPU usage by running the `nvidia-smi` command on the master lab node. After a few seconds, when the nodes are allocated and the script begins its execution, we should see the GPUs 0,1,2,3 utilized, as shown below. Please notice that if it's necessary to wait until the actual training starts. Until then, you will not be able to see any GPU activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/2N_4gpus_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 21:33:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   53C    P0             310W / 300W |  79551MiB / 81920MiB |     81%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   56C    P0             306W / 300W |  79551MiB / 81920MiB |     85%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   50C    P0             281W / 300W |  79551MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   51C    P0             307W / 300W |  79551MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU utilization on the master node\n",
    "!sleep 60\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the NeMo GPT-3 pretraining, we can check the generated [logs](./nemo/logs/2Nodes4GPUS.txt) during execution.\n",
    "\n",
    "Let's first verify the world size of our run. We should see this:\n",
    "```\n",
    "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
    "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
    "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
    "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n"
     ]
    }
   ],
   "source": [
    "!grep \"Initializing distributed:\" /dli/nemo/logs/2Nodes4GPUS.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the performance of the GPT pretraining on 4 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  19%|█▊        | 28/150 [00:34<02:31,  1.24s/it, loss=10.5, v_num=, reduced_train_loss=10.30, global_step=19.00, consumed_samples=1216.0]\n",
      "Epoch 0:  19%|█▉        | 29/150 [00:34<02:25,  1.20s/it, loss=10.5, v_num=, reduced_train_loss=10.30, global_step=19.00, consumed_samples=1216.0]\n",
      "Epoch 0:  20%|██        | 30/150 [00:35<02:20,  1.17s/it, loss=10.5, v_num=, reduced_train_loss=10.30, global_step=19.00, consumed_samples=1216.0, val_loss=10.20]\n",
      "Epoch 0, global step 20: 'val_loss' reached 10.20249 (best 10.20249), saving model to '/dli/nemo/2Nodes4GPUS/checkpoints/megatron_gpt--val_loss=10.20-step=20-consumed_samples=1216.0.ckpt' as top 10\n"
     ]
    }
   ],
   "source": [
    "!cat /dli/nemo/logs/2Nodes4GPUS.txt | grep Epoch | tail -4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract logs, we can see the training performance while using 4 GPUs (2 GPUs per node). \n",
    "\n",
    "```\n",
    "Epoch 0: 100%|██████████| 150/150 [02:14<00:00,  1.12it/s, loss=8.43, v_num=, reduced_train_loss=8.370, global_step=99.00, consumed_samples=6336.0, val_loss=8.360]\n",
    "```\n",
    "Compare this with 1 node executions, and discuss it with the instructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.3 Inter/Intra Node Communications \n",
    "\n",
    "In the previous run, you may have noticed that we added the NCCL variable `NCCL_DEBUG=INFO` in order to output the NCCL debug log traces during training as shown bellow. \n",
    "```\n",
    "slurmnode1:2326:2509 [1] NCCL INFO Channel 00/0 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
    "slurmnode1:2326:2509 [1] NCCL INFO Channel 01/0 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
    "slurmnode1:2326:2509 [1] NCCL INFO Channel 00 : 1[200000] -> 0[100000] via SHM/direct/direct\n",
    "slurmnode1:2326:2509 [1] NCCL INFO Channel 01 : 1[200000] -> 0[100000] via SHM/direct/direct\n",
    "```\n",
    "Let's check what NCCL reported in the generated [logs](./nemo/logs/2Nodes4GPUS.txt) during the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 00/0 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 01/0 : 1[200000] -> 2[300000] [send] via NET/Socket/0\n",
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 00/0 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 01/0 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 00/02 :    0   1   2   3\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 01/02 :    0   1   2   3\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 00/0 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 01/0 : 3[400000] -> 0[100000] [receive] via NET/Socket/0\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 00/0 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 01/0 : 0[100000] -> 1[200000] via P2P/IPC/read\n"
     ]
    }
   ],
   "source": [
    "!grep Channel /dli/nemo/logs/2Nodes4GPUS.txt | grep slurmnode1 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow us to check the type of networking used between GPUs and nodes during the training. \n",
    "\n",
    "The internode communications are reported as `NET/Socket/0` while direct GPU-to-GPU communications are reported as `P2P/IPC`.\n",
    "In our configuration, direct GPU-to-GPU communication should be used within nodes: GPU0<->GPU1 and GPU2<->GPU3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 00/0 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode1:3419:3604 [1] NCCL INFO Channel 01/0 : 1[200000] -> 0[100000] via P2P/IPC/read\n",
      "slurmnode2:160:342 [1] NCCL INFO Channel 00/0 : 3[400000] -> 2[300000] via P2P/IPC/read\n",
      "slurmnode2:160:342 [1] NCCL INFO Channel 01/0 : 3[400000] -> 2[300000] via P2P/IPC/read\n",
      "slurmnode2:159:344 [0] NCCL INFO Channel 00/0 : 2[300000] -> 3[400000] via P2P/IPC/read\n",
      "slurmnode2:159:344 [0] NCCL INFO Channel 01/0 : 2[300000] -> 3[400000] via P2P/IPC/read\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 00/0 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode1:3418:3602 [0] NCCL INFO Channel 01/0 : 0[100000] -> 1[200000] via P2P/IPC/read\n",
      "slurmnode2:159:358 [0] NCCL INFO Channel 00/0 : 2[300000] -> 3[400000] via P2P/IPC/read\n",
      "slurmnode2:159:358 [0] NCCL INFO Channel 01/0 : 2[300000] -> 3[400000] via P2P/IPC/read\n"
     ]
    }
   ],
   "source": [
    "!grep Channel /dli/nemo/logs/2Nodes4GPUS.txt | grep P2P/IPC | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.4 Monitoring and Profiling Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, monitoring the training runs with NeMo Framework was done via the text log files. However, monitoring training is also possible through the tensorboard visualization of the hyperparameters and training/evaluation metrics. In addition, visualizing can be helpful for debugging and optimizing the models during training.\n",
    "\n",
    "## 3.4.1 Training Metrics Visualization on Tensorboard\n",
    "\n",
    "<img src=\"images/tensorboard1.png\" width=\"1024\"/>\n",
    "\n",
    "In the previous NeMo Framework runs, we set the arguments for recording Tensorboard events. The graphs of all the previous experiments are available in the folder `nemo`.\n",
    "\n",
    "Note that to profile memory usage it's necessary to add a profiler to the PyTorch Lightning trainer, used internally by NeMo Framework. This can be done by adding the following lines to `megatron_gpt_pretraining.py`:\n",
    "```\n",
    "    if cfg.trainer.get('use_profiler', False):\n",
    "        schedule = torch.profiler.schedule(wait=50, warmup=1, active=2, repeat=1)\n",
    "        profiler = PyTorchProfiler(activities=[\n",
    "                torch.profiler.ProfilerActivity.CPU,\n",
    "                torch.profiler.ProfilerActivity.CUDA,\n",
    "            ], \n",
    "            schedule=schedule, record_shapes=True, profile_memory=True, with_stack=True, with_flops=True, with_modules=True)\n",
    "        del cfg.trainer.use_profiler\n",
    "        trainer = Trainer(plugins=plugins, strategy=strategy, profiler=profiler, **cfg.trainer)\n",
    "    else:\n",
    "        trainer = Trainer(plugins=plugins, strategy=strategy, **cfg.trainer)\n",
    "```\n",
    "\n",
    "You can also add profiler via default option `+trainer.profiler=pytorch` (or `+trainer.profiler=simple`, `trainer.profiler=advanced`), but it will output less information.\n",
    "\n",
    "Execute the next cell to create a link to Tensorboard for the browser. Then, click the link to see graphs of experiment metrics saved in the specified directory with `tensorboard` logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4.2 Pytorch Profiler with Tensorboard\n",
    "\n",
    "Several existing profiling tools can be used to investigate bottlenecks during the training and inference processes. This allows us to identify the most expensive operations, issues such as GPU starvation, or unnecessary operations. \n",
    "\n",
    "In this class, we will use the [Pytorch Lightning Profiler](https://pytorch-lightning.readthedocs.io/en/1.5.10/advanced/profiler.html), a tool collecting performance metrics during Pytorch executions. The [Tensorboard-plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html) with the PyTorch Profiler provides a visualization of each GPU process, with several measures of operations running on each GPU, whether using hardware accelerator (TensorCores). It also provides recommendations to improve the process.\n",
    "\n",
    "We will trace only the execution of the GPU_0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the previous run, profiling is available on the Tensorboard link at the `PYTORCH_PROFILER` tab.\n",
    "\n",
    "<img src=\"images/profiling1.png\" width=\"1280\"/>\n",
    "\n",
    "In case you already closed the Tensorboard page, you can re-generate the link by executing the next cell. Click the link to open Tensorboard and then, go to the `PYTORCH_PROFILER` tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiler homepage shows an overview of tracing. \n",
    "\n",
    "### The Memory View\n",
    "Let's have a look at the Memory View showing the memory allocation and deallocation of our run. We can zoom into the memory curve to see the related memory events.\n",
    "\n",
    "<img src=\"images/profiling4.png\" width=\"1280\"/>\n",
    "\n",
    "The memory allocation followed by deallocation corresponds to the forward and backward pass. This process is traced twice as we traced 2 training steps. We can also see that a peak of ~73GB of the GPU device memory is used during the training step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\\\n",
    "Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints\n",
    "!rm -rf /dli/nemo/2Nodes4GPUS/checkpoints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.5 Increase the Batchsize / GPU\n",
    "\n",
    "**Special Warning:** In this section, Out Of Memory (OOM) issues are expected! No problem, we will see a few solutions addressing this problem in the next notebook.\n",
    "\n",
    "\n",
    "The size of the current GPT model fits into the GPU memory and thus does not require model distribution (tensor or pipeline parallelism). \\\n",
    "Using only data parallelism, we can improve the training throughput (number of sequences processed per second) by increasing the batch size processed per GPU until reaching the maximum batch size that fits on GPU memory.\\\n",
    "Let's check this by increasing the data processed by each GPU. When using 4 GPUs with only data parallelism (DP_SIZE $= 4$), by increasing the micro batch size per GPUs from 16 to 32, the global batch size should be MICRO_BATCH_SIZE $\\times$ DP_SIZE $= 128$.\n",
    "\n",
    "Let's prepare the training script for this scenario by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_32.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_32.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=2       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=32      # <--- CHANGED HERE\n",
    "GLOBAL_BATCH_SIZE=128    # <--- CHANGED HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"2Nodes4GPUS_DP_4_MBS_32\"\n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=32 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "            +trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_DP_4_MBS_32.sh](./code/pretrain_gpt_2Node4GPU_DP_4_MBS_32.sh) and check the SLURM queue using the squeue command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 13\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                13  slurmpar dli_2nod     root  R       0:00      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_DP_4_MBS_32.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the NeMo GPT-3 pretraining performance by looking at the generated [logs](./nemo/logs/2Nodes4GPUS_DP_4_MBS_32.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/150 [00:00<?, ?it/s] [NeMo I 2024-03-21 21:37:45 indexed_dataset:454]     reading sizes...\n",
      "Epoch 0:   0%|          | 0/150 [00:17<?, ?it/s]slurmnode1:5489:5684 [0] NCCL INFO [Service thread] Connection closed by localRank 0\n"
     ]
    }
   ],
   "source": [
    "!sleep 60\n",
    "!grep Epoch /dli/nemo/logs/2Nodes4GPUS_DP_4_MBS_32.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "No elapsed time per iteration is shown. **What just happened?!** \n",
    "\n",
    "Are we saturating the GPU memory? Run the cell bellow to search for RuntimeError in our logs. You should see:\n",
    "```\n",
    "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 1; 79.35 GiB total capacity; 70.19 GiB already allocated; 3.61 GiB free; 74.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 79.35 GiB total capacity; 70.19 GiB already allocated; 3.61 GiB free; 74.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 1; 79.15 GiB total capacity; 76.99 GiB already allocated; 135.25 MiB free; 78.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 79.15 GiB total capacity; 76.99 GiB already allocated; 135.25 MiB free; 78.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 79.15 GiB total capacity; 76.99 GiB already allocated; 135.25 MiB free; 78.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 1; 79.15 GiB total capacity; 76.99 GiB already allocated; 135.25 MiB free; 78.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!grep \"torch.cuda.OutOfMemoryError\" /dli/nemo/logs/2Nodes4GPUS_DP_4_MBS_32.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Indeed, we should see `CUDA out of memory` errors which means that the GPU memory cannot handle training this transformer model size with the specified arguments.\n",
    "\n",
    "Doubling the amount of data processed per GPU is too much for the 80Gb of memory.  \n",
    "\n",
    "In the next lab, we will focus on how to address this issue by reducing the model's memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints and logs directory\n",
    "!rm -rf /dli/nemo/2Nodes4GPUS_DP_4_MBS_32/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.6 Exercise: Hybrid Distributed Training Strategy \n",
    "\n",
    "\n",
    "Let's configure a new NeMo GPT pretraining execution on 2 nodes (4 GPUs) with both tensor and pipeline parallel by modifying the \"FIXME\" in the following cell. \n",
    "\n",
    "To use tensor and pipeline parallel, we can keep distributed the model using in 2 dimensions: 2 GPUs for tensor parallelism and 2 GPUs for pipeline parallelism. Thus, no more resources will remain for the data parallel. In this case, the `GLOBAL_BATCH_SIZE` should be downgraded to the same size as the `MICRO_BATCH_SIZE`. This would allow us to train the model with a micro batch size of 32.\n",
    "\n",
    "If you get stuck, view the [solution](./solutions/ex3.4.ipynb) for a hint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes_hybrid\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=2       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=2        # <--- CHANGE HERE\n",
    "PP_SIZE=2         # <--- CHANGE HERE\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=32         # <--- CHANGE HERE\n",
    "GLOBAL_BATCH_SIZE=32         # <--- CHANGE HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"2Nodes4GPUS_hybrid\"       # <--- CHANGED HERE \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=32 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "            +trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_hybrid.sh](./code/pretrain_gpt_2Node4GPU_hybrid.sh) for hybrid multi-node run and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 14\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                14  slurmpar dli_2nod     root PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 nodes jobs\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_hybrid.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To understand the performance of the NeMo GPT-3 pretraining, we can check the generated logs during the execution.\n",
    "\n",
    "Let's first look at the generated [logs](./nemo/logs/2Nodes4GPUS_hybrid.txt) and check world size of our executed hybrid run. We should see this:\n",
    "\n",
    "```\n",
    "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
    "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
    "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
    "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    }
   ],
   "source": [
    "!sleep 20\n",
    "!grep \"Initializing distributed:\" /dli/nemo/logs/2Nodes4GPUS_hybrid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the training performance and GPU0 memory allocation and deallocation of our run. You should see a graph similar to the following: \n",
    "\n",
    "<img src=\"images/profiling_hybrid.png\" width=\"1280\"/>\n",
    "\n",
    "What is the constant time step between the forward and backward pass? Discuss with the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep Epoch /dli/nemo/logs/2Nodes4GPUS_hybrid.txt | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/nemo/2Nodes4GPUS_hybrid/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                14  slurmpar dli_2nod     root  R       0:32      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                14  slurmpar dli_2nod     root CG       0:36      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will see how to optimize the GPT pretraining using techniques such as Mixed Precision, Gradient Accumulation and Activation Checkpointing. Move on to [04_GPT_LM_pretrainings_optimizations.ipynb](04_GPT_LM_pretrainings_optimizations.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
