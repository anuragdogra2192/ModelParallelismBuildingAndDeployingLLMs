{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Distributed Training Optimization\n",
    "\n",
    "In this notebook, we will learn how to quantify the [NeMo](https://github.com/NVIDIA/NeMoM) GPT pretraining performance and see optimization techniques such as Mixed Precision, Gradient Accumulation and Activation Checkpointing.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Optimize multi-node training of NeMo Framework scripts using Automatic Mixed Precision and Activation Checkpointing \n",
    "* Understand how to compute training performance\n",
    "\n",
    "\n",
    "**[4.1 Mixed Precision Training](#4.1-Mixed-Precision-Training)<br>**\n",
    "**[4.2 Activation Checkpointing](#4.2-Activation-Checkpointing)<br>**\n",
    "**[4.3 Gradient Accumulation](#4.3-Gradient-Accumulation)<br>**\n",
    "**[4.4 The Training Performance](#4.4-The-Training-Performance)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.1 Compute the Number of Parameters of Transformer Model](#4.4.1-Compute-the-Number-of-Parameters-of-Transformer-Model)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.2 Exercise: Compute the Number of Parameters For Our Model](#442-exercise-compute-the-number-of-parameters-for-our-model)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.3 Compute the Theoretical Peak FLOP per second per GPU](#443-compute-the-theoretical-peak-flop-per-second-per-gpu)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[4.4.4 Estimate the Training Duration / Epoch](#444-estimate-the-training-duration--epoch)<br>\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs with the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.1 Mixed Precision Training \n",
    "\n",
    "<img src=\"images/AMP.png\" width=\"700\"/>\n",
    "\n",
    "**Automatic Mixed Precision (AMP)** allows using different numerical precisions when running mathematical operations. It performs some operations in half-precision format reducing the memory required while keeping single-precision in critical parts of the network.\n",
    "\n",
    "Training with Automatic Mixed Precision takes advantage of the hardware acceleration provided by Tensor Cores available in NVIDIA GPUs starting from Volta architecture (Volta, Turing, Ampere, Hopper, and future architectures). To learn more about training with AMP, refer to the [Mixed precision training documentation](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, Mixed Precision training was not enabled for the baseline run.  We can confirm that by examining the `HPARAMS` (hyperparameters) tab in TensorBoard.\n",
    "\n",
    "<img src=\"images/profiling_hparams.png\" width=\"1000\"/>\n",
    "\n",
    "We can speed up the operations by enabling Automatic Mixed Precision with FP16.\n",
    "\n",
    "To run NeMo GPT pretraining in Mixed Precision, simply update the argument `trainer.precision=16` in the `TRAINER_ARGS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=2       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1 \n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=16      \n",
    "GLOBAL_BATCH_SIZE=64\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"2Nodes4GPUS_increase_MBS_fp16\"        \n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=16 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "            +trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh](./code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 15\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                15  slurmpar dli_2nod     root  R       0:00      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# submit the 2 node job\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the NeMo GPT3 pretraining, we can check the generated logs during execution.\n",
    "\n",
    "Let's first look at the generated [logs](./nemo/logs/2Nodes4GPUS_increase_MBS_fp16.txt) and check the world size of our executed run. We should see this:\n",
    "\n",
    "```\n",
    "trainer:\n",
    "...\n",
    "    precision: 16\n",
    "...\n",
    "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
    "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
    "...\n",
    "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
    "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "Notice that we are running in fp16 mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few seconds, let's check the training performance and discuss that with the instructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  48%|████▊     | 72/150 [01:00<01:05,  1.19it/s, loss=10.1, v_num=, reduced_train_loss=9.920, global_STAGE:2024-03-21 21:53:56 5707:5707 ActivityProfilerController.cpp:300] Completed Stage: Collection\n",
      "Epoch 0:  50%|█████     | 75/150 [01:04<01:04,  1.16it/s, loss=10.1, v_num=, reduced_train_loss=9.870, global_step=53.00, consumed_samples=3392.0, val_loss=10.10]\n"
     ]
    }
   ],
   "source": [
    "!sleep 60\n",
    "!grep Epoch /dli/nemo/logs/2Nodes4GPUS_increase_MBS_fp16.txt | tail -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Profiler with Tensorboard\n",
    "\n",
    "The profiling is available on the Tensorboard link at the `PYTORCH_PROFILER` tab for the previous run. In case you already closed the Tensorboard page, you can re-generate the link by executing the next cell. Click the link to open Tensorboard and then go to the `PYTORCH_PROFILER` tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "const href = window.location.hostname +'/tensorboard/';\n",
       "let a = document.createElement('a');\n",
       "let link = document.createTextNode('Open Tensorboard!');\n",
       "a.appendChild(link);\n",
       "a.href = \"http://\" + href;\n",
       "a.style.color = \"navy\"\n",
       "a.target = \"_blank\"\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "const href = window.location.hostname +'/tensorboard/';\n",
    "let a = document.createElement('a');\n",
    "let link = document.createTextNode('Open Tensorboard!');\n",
    "a.appendChild(link);\n",
    "a.href = \"http://\" + href;\n",
    "a.style.color = \"navy\"\n",
    "a.target = \"_blank\"\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `2Nodes4GPUS_increase_MBS_fp16` results, we see that the GPU utilization is reduced. \n",
    "\n",
    "<img src=\"images/profiling_memory_util.png\" width=\"700\"/>\n",
    "\n",
    "From the GPU Kernel view, we can see that when enabling Mixed precision, 17.2% of GPU operations are accelerated with TensorCores.\n",
    "\n",
    "<img src=\"images/profiling5_AMP.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\n",
    "### How about the memory?\n",
    "We can see that the memory consumption decreased with a peak of \\~45G (~73G in the baseline). This is because half-precision floating point format (FP16) uses 16 bits, compared to 32 bits for single precision (FP32). Lowering the required memory enables training larger models or training with larger mini-batches.\n",
    "\n",
    "<img src=\"images/profiling_FP16_memory.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/nemo/2Nodes4GPUS_increase_MBS_fp16/checkpoints/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.2 Activation Checkpointing \n",
    "\n",
    "\n",
    "<img src=\"images/activation_checkpoiting.png\" width=\"700\" align=\"center\"/>\n",
    "\n",
    "Activation Checkpointing is another technique allowing us to save memory during the training with the cost of additional re-compute. In the vanilla forward and backward pass, all feature maps are computed during the forward pass and stored for the backward step. In the activation checkpointing strategy, only some intermediate results are stored (called checkpoints) during the forward pass. Those checkpoints will be used at the backwards pass to recompute further feature maps when needed. There are several implementations of checkpointing, including manual checkpoints provided by the user or automatic selection.\n",
    "\n",
    "NeMo Framework supports two activation checkpointing methods: uniform and block.\n",
    "\n",
    "- Uniform: Uniformly divides the layers into groups and stores the input activations of each group in memory. \n",
    "- Block: Checkpoints the input activations of a set number of individual Transformer layers per pipeline stage.\n",
    "\n",
    "To run NeMo GPT pretraining with Activation Checkpointing, simply set the argument `model.activations_checkpoint_method` to `uniform` or `block`, `model.activations_checkpoint_granularity` to `full` or `selective`, and update `model.activations_checkpoint_num_layers` (usually it's sufficient to set it to 1) in `GPT_ARGS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=2       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=16     \n",
    "GLOBAL_BATCH_SIZE=64    \n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing\"        \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=16 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "            +trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            model.activations_checkpoint_method=uniform \\\n",
    "            model.activations_checkpoint_granularity=full \\\n",
    "            model.activations_checkpoint_num_layers=1 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing.sh](./code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpoiting.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 16\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                16  slurmpar dli_2nod     root PD       0:00      2 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 node job\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the NeMo GPT3 pretraining, we can check the generated [logs](./nemo/logs/2Nodes4GPUS_increase_MBS_fp16_activation_checkpoiting.txt) and discuss them with the instructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|██        | 30/150 [00:32<02:11,  1.10s/it, loss=10.8, v_num=, reduced_train_loss=10.80, global_step=19.00, consumed_samples=1216.0, val_loss=10.70]\n",
      "Epoch 0, global step 20: 'val_loss' reached 10.68408 (best 10.68408), saving model to '/dli/nemo/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing/checkpoints/megatron_gpt--val_loss=10.68-step=20-consumed_samples=1216.0.ckpt' as top 10\n"
     ]
    }
   ],
   "source": [
    "!sleep 60\n",
    "!grep Epoch /dli/nemo/logs/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing.txt| tail -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the memory?\n",
    "\n",
    "The profiling is available on the Tensorboard link at the `PYTORCH_PROFILER` tab. When using activation checkpointing, we can see that for `2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing` the memory consumption for a single GPU is reduced considerably to a peak of ~10G. This is since some activations are not stored and recomputed when necessary. Also notice the increase in the time for a single training step.\n",
    "\n",
    "\n",
    "By zooming into the graph, we can trace the Pytorch `CheckpointFunctions`. \n",
    "\n",
    "<img src=\"images/profiling_FP16_checkpoiting_memory.png\" width=\"1024\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/nemo/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.3 Gradient Accumulation \n",
    "\n",
    "\n",
    "Another way of increasing the batch size is to use gradient accumulation. Instead of splitting the data across workers as in Distributed Data Parallel, in the Gradient Accumulation technique the same worker processes several batches and accumulates the gradients before updating the model parameters.\n",
    "\n",
    "[NVIDIA APEX Library](https://github.com/NVIDIA/apex) provides an optimized implementation when using Gradient Accumulation with Automatic Mixed precision. This implementation removes unnecessary double precision copies of the gradient by accumulating it in low precision first before going back to double precision. NeMo Framework uses the APEX implementation.  \n",
    "\n",
    "\n",
    "To run NeMo GPT pretraining with Gradient Accumulation, simply increase the global batch size while maintaining the micro batch size the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.sh\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH --nodes=2\n",
    "#SBATCH --ntasks-per-node=2       \n",
    "#SBATCH --cpus-per-task=32 ### Number of threads per task (OMP threads)\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "set -x -e\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=2\n",
    "GPUS_PER_NODE=2\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=16     \n",
    "GLOBAL_BATCH_SIZE=256   \n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation\"        \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "\n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=16 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "            +trainer.use_profiler=true \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            model.activations_checkpoint_method=uniform \\\n",
    "            model.activations_checkpoint_granularity=full \\\n",
    "            model.activations_checkpoint_num_layers=1 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "clear; srun --jobid $SLURM_JOBID bash -c 'NCCL_DEBUG=INFO $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's submit the previous sbatch script [pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.sh](./code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.sh) and check the SLURM queue using the `squeue` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 17\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                17  slurmpar dli_2nod     root PD       0:00      2 (Resources)\n",
      "                16  slurmpar dli_2nod     root  R       1:16      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Submit the 2 node job\n",
    "!sbatch /dli/code/pretrain_gpt_2Node4GPU_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.sh\n",
    "\n",
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the NeMo GPT3 pretraining, we can check the generated [logs](./nemo/logs/log_2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.txt) and discuss them with the instructor.\n",
    "\n",
    "Let's compare the number of micro-batches per GPU when enabling or disabling Gradient Accumulation. To do so, let’s compare to the previous runs without Gradient Accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without gradient accumulation:\n",
      "          constant_steps: 50000\n",
      "24-03-21 21:56:37 - PID:11253 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 1\n",
      "    constant_steps: 50000\n",
      "\n",
      "With 4x gradient accumulation:\n",
      "          constant_steps: 50000\n",
      "24-03-21 21:59:13 - PID:13200 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 4\n",
      "    constant_steps: 50000\n"
     ]
    }
   ],
   "source": [
    "!sleep 60\n",
    "\n",
    "print(\"Without gradient accumulation:\")\n",
    "!grep constant /dli/nemo/logs/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing.txt\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"With 4x gradient accumulation:\")\n",
    "!grep constant /dli/nemo/logs/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the training performance and the number of consumed samples with Gradient Accumulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 80: 'val_loss' reached 8.91388 (best 8.91388), saving model to '/dli/nemo/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation/checkpoints/megatron_gpt--val_loss=8.91-step=80-consumed_samples=20224.0.ckpt' as top 10\n",
      "Epoch 0:  86%|████████▌ | 129/150 [04:00<00:39,  1.87s/it, loss=9.02, v_num=, reduced_train_loss=8.660, global_step=87.00, consumed_samples=22272.0, val_loss=8.910]\n"
     ]
    }
   ],
   "source": [
    "!grep Epoch /dli/nemo/logs/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation.txt | tail -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the memory?\n",
    "\n",
    "The profiling is available in Tensorboard link at the `PYTORCH_PROFILER` tab under the run `2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation`. \n",
    "\n",
    "We can see the 4 gradient accumulations stages per step on the memory tracing. \n",
    "\n",
    "<img src=\"images/profiling_FP16_checkpoiting_gradient_acc_memory.png\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints \n",
    "!rm -rf /dli/nemo/2Nodes4GPUS_increase_MBS_fp16_activation_checkpointing_gradient_accumulation/checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4.4 The Training Performance\n",
    "\n",
    "In order to train large Neural Networks in a reasonable time, scaling the infrastructure is unavoidable. \n",
    "Let's first compute the number of parameters of our transformer models and estimate its training according to the hardware infrastructure and the experimentally observed training throughput.\n",
    "\n",
    "\n",
    "## 4.4.1 Compute the Number of Parameters of Transformer Model\n",
    "\n",
    "The number of parameters for a Transformers model is computed as:\n",
    "$P = 12 l h^2 (1 + \\frac{13}{12h} + \\frac{V+s}{12lh})$ where:\n",
    "- $l$ = Number of Layers\n",
    "- $h$ = Hidden Size\n",
    "- $V$ = Vocabulary Size\n",
    "- $s$ = Sequence Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parameters of the Transformers model\n",
    "def calculate_number_parameters(l,h,s,V):\n",
    "    # Compute the number of parameters of the model\n",
    "    P=12*l*h*h *(1+ (13/(12*h)) + ((V+s)/(12*l*h)))\n",
    "    print(\"The number of parameters for the GPT architecture is: {} \\n\".format(int(P)))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's compute the number of parameters of the transformer model having 40 layers, a hidden size of 6144, vocabulary size of 50257 and sequence length of 1024. This model should be approximately 18 billion parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters for the GPT architecture is: 18437806080 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model architecture parameters\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "P=calculate_number_parameters(l,h,s,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.2 Exercise: Compute the Number of Parameters For Our Model\n",
    "\n",
    "Compute the number of parameters of the model we have been experimenting with in previous notebooks.\n",
    "Have a look at the model architecture arguments in any NeMo GPT pretraining scripts from previous notebooks. \n",
    "\n",
    "The vocabulary size is not specified explicitly in scripts and is also **50257** for NeMo GPT.\n",
    "\n",
    "If you get stuck, you can look at the [solution](solutions/ex4.1.2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters for the GPT architecture is: 124438272 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124438272.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our model architecture parameters\n",
    "l=12\n",
    "h=768\n",
    "s=1024\n",
    "V=50257\n",
    "calculate_number_parameters(l,h,s,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.3 Compute the Theoretical Peak FLOP per second per GPU\n",
    "\n",
    "As detailed in the paper [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf), the majority of floating-point operations in the model are performed in the matrix multiplications (GEMMs). If we consider only these GEMMs operations, the number of FLOPs per iteration is $F = 96 B s l h^2 (1 + \\frac{s}{6h} + \\frac{V}{16lh})$ where $B$ is the batch size. \n",
    "\n",
    "And, in case we have an estimate of the time spent per iteration `time_per_iteration_second`, it is possible then to compute the theoretical peak FLOP per second and per GPUs and estimate the GPU usage by comparing. \n",
    "\n",
    "The following table shows the training performance of several GPT model sizes (from 1.7B to 1 trillion) pretrained on a SuperPOD cluster with A100 GPUs.\n",
    "<img src=\"https://github.com/NVIDIA/Megatron-LM/blob/main/images/cases_april2021.png?raw=true\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical peak FLOP per second per GPU - with activation checkpointing (2 forwards and 1 backward)\n",
    "def calculate_theoretical_peak_FLOP_s_GPU(B, s, l, h, number_GPUs, time_per_iteration_second):\n",
    "    # The number of FLOPs per iteration\n",
    "    F = 96*B*s* l*h*h *(1 + s/ (6*h) + V/(16*l*h))/1e+12\n",
    "    \n",
    "    #Theoretical peak FLOP per second per GPU\n",
    "    PF= (F/time_per_iteration_second/number_GPUs)\n",
    "    print(\"Theoretical peak FLOP/s/GPU: {}\\n\".format(PF))\n",
    "    \n",
    "    # Percentage of theoretical peak FLOP/s on a A100 FP16 (change according the hardware)\n",
    "    GPU_usage= PF/ 312 *100\n",
    "    print(\"Percentage of theoretical peak FLOP/s: {}%\".format(GPU_usage))\n",
    "    \n",
    "    return PF, GPU_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of theoretical peak FLOP/s in the previous function is based on **A100** hardware capabilities in **FP16/BF16 which is 312**. This needs to be updated according to the corresponding Tensor Core GPU performance specs. To learn more about the Ampere and newer Hopper architecture specifications, follow the following links: [Ampere](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/), [Hopper](https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/).\n",
    "\n",
    "\n",
    "If we consider our previous 18B parameters models pretrained on 16 GPUs with a global batch size of 512, they have a time per iteration of 32.09s. \n",
    "Let's compute the theoretical peak FLOP per second per GPU and the percentage GPU utilization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical peak FLOP/s/GPU: 157.7296879710454\n",
      "\n",
      "Percentage of theoretical peak FLOP/s: 50.55438717020686%\n"
     ]
    }
   ],
   "source": [
    "global_batch_size=512\n",
    "number_GPUs=16\n",
    "time_per_iteration_second=32.09\n",
    "\n",
    "# Considering the 18B parameters model\n",
    "l=40\n",
    "h=6144\n",
    "s=1048\n",
    "V=50257\n",
    "    \n",
    "PF,GPU_usage=calculate_theoretical_peak_FLOP_s_GPU(global_batch_size, s, l, h, number_GPUs, time_per_iteration_second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4.4 Estimate the Training Duration / Epoch\n",
    "\n",
    "It is possible to estimate training duration per epoch according to the model, dataset, and hardware size. Training time (in seconds) is approximated with this equation $\\approx \\frac{8*T*P}{n * PF}$ where: \n",
    "- $T$ = Number of tokens in the dataset\n",
    "- $P$ = Numbers of parameters \n",
    "- $n$ = Number of GPUs\n",
    "- $PF$ = Achieved teraFLOP/s per GPU\n",
    "\n",
    "More details are described in the paper [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf).\n",
    "\n",
    "Let's execute the 2 following cells to estimate the training duration for the 18B parameters models trained on a dataset of $T$=300 billion tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "# Estimate the training time\n",
    "def estimate_days_needed(T, P, N, PF):  \n",
    "    compute_sec=8*T*P/(N*PF*10e12)\n",
    "    # Convert compute seconds to days\n",
    "    to_days=round(compute_sec/(3600*24))\n",
    "    print(\"This language model will need {} days per epoch.\".format(colored(str(to_days),'blue', attrs=['bold'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This language model will need \u001b[1m\u001b[34m203\u001b[0m days per epoch.\n"
     ]
    }
   ],
   "source": [
    "# Number of tokens in the dataset\n",
    "T=300*10e09\n",
    "\n",
    "estimate_days_needed(T, P, number_GPUs, PF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 203 days, which is almost **7 months** required to train the 18B model on 16 GPUs (2 nodes) with a dataset of 300B tokens! \n",
    "\n",
    "In this case, scaling the number of nodes is unavoidable in order to train the model in a reasonable amount of time. \n",
    "\n",
    "For instance, consider a GPT-3 model with $P$=175 billion parameters trained on a dataset of $T$=300 billion tokens on $n$=1024 A100 GPUs. Using a batch size of 1536, we achieve $F$=140 teraFLOP/s per GPU. Thus, the time required to train this model is **34 days**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, we will have an introduction to [NeMo Framework Launcher](https://github.com/NVIDIA/NeMo-Megatron-Launcher), designed to be a simple and easy to use tool for launching NeMo Framework training jobs on CSPs or on-prem clusters. Move on to [05_NeMo_Framework_Launcher.ipynb](05_NeMo_Framework_Launcher.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
