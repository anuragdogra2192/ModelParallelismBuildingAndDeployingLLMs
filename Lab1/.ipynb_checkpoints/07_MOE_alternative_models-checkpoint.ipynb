{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Mixture of Experts (MoE)\n",
    "\n",
    "In this notebook, we will learn about Mixture of Experts model training.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are:\n",
    "* Learn how to incorporate linear experts on a simple Convolutional Network\n",
    "* Learn how to train the new Mixture of Experts CNN for classification\n",
    "\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NCCL_P2P_DISABLE=1\n"
     ]
    }
   ],
   "source": [
    "# disable P2P connections for single-node DeepSpeed runs\n",
    "%env NCCL_P2P_DISABLE=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7.1 Mixture of Experts Introduction\n",
    "\n",
    "A Mixture of Experts (MoE) is a neural network where some layers are partitioned into small groups that can be activated or not according to the context. \n",
    "This structure allows the network to learn a wider range of behaviors. The other advantage is that MoE models will require less computation as only few experts are active at a time.\n",
    "\n",
    "<img src=\"images/MOE.png\" width=\"450\" />\n",
    "\n",
    "In the recent literature, several models have been developed following the MoE structure, such as the [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Write the Mixture of Experts for the basline CNN\n",
    "\n",
    "Let's go back to our CNN cifar-10  classifier model. Let's modify it to add 1 MoE layer. The convolutional layers of the CNN extract features, while the later fully connected layers are specialized for the CIFAR-10 classification problem. \n",
    "To add expert layers in the network definition, use the `deepspeed.moe.layer.MoE` as follows (modify the forward pass accordingly):\n",
    "\n",
    "```\n",
    "deepspeed.moe.layer.MoE( hidden_size=<Hidden dimension of the model>, \n",
    "                         expert=<Torch module that defines the expert>, \n",
    "                         num_experts=<Desired number of expert>, \n",
    "                         ep_size=<Desired expert-parallel world size>,\n",
    "                         ...\n",
    "                         )\n",
    "                         \n",
    "```\n",
    "\n",
    "Learn more about the DeepSpeed Mixture of Experts in the [dedicated DeepSpeed documentation.](https://deepspeed.readthedocs.io/en/latest/moe.html) \n",
    "\n",
    "Let's transform the latest fully connected layer `fc3` to a MoE layer in order to evaluate the features extracted from early layers. We will add a final classifier `fc4`.\n",
    "We already prepared the [cifar10_deepspeed_MOE.py](./code/moe/cifar10_deepspeed_MOE.py) script. Letâ€™s run it using 8 experts partitioned on 4 GPUs, which means that each GPU will handle 2 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-22 01:17:41,651] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-03-22 01:17:41,702] [INFO] [runner.py:457:main] cmd = /usr/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 /dli/code/moe/cifar10_deepspeed_MOE.py --deepspeed --deepspeed_config /dli/code/moe/ds_config.json --moe --ep-world-size 4 --num-experts-per-layer 8 --top-k 1 --noisy-gate-policy RSample --moe-param-group --profile-execution=True --profile-name=zero0_MOE\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.15.5\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2024-03-22 01:17:43,506] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/usr/local/lib/python3.8/dist-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "[2024-03-22 01:17:45,885] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[2024-03-22 01:17:49,789] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 2 | expert_parallel_size: 4\n",
      "[2024-03-22 01:17:49,791] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2024-03-22 01:17:49,815] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert and data parallel groups with size 4\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "No existing process group found, creating a new group named: ep_size_4No existing process group found, creating a new group named: ep_size_4\n",
      "\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2024-03-22 01:17:51,164] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [0]\n",
      "[2024-03-22 01:17:51,174] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [1]\n",
      "[2024-03-22 01:17:51,185] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [2]\n",
      "[2024-03-22 01:17:51,185] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [3]\n",
      "[2024-03-22 01:17:51,196] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [0, 1, 2, 3]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:433: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:433: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:433: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:433: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\n",
      "  warnings.warn(\n",
      "[2024-03-22 01:17:51,797] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10085320472717285 seconds\n",
      "[2024-03-22 01:17:52,497] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-03-22 01:17:52,497] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-03-22 01:17:52,497] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2024-03-22 01:17:52,508] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-03-22 01:17:52,508] [INFO] [engine.py:785:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-03-22 01:17:52,508] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f746945a6d0>\n",
      "[2024-03-22 01:17:52,509] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2024-03-22 01:17:52,509] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "[2024-03-22 01:17:52,509] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-03-22 01:17:52,509] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-03-22 01:17:52,509] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2024-03-22 01:17:52,509] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2024-03-22 01:17:52,510] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 32768\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupLR\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   steps_per_print .............. 2000\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   train_batch_size ............. 16\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  4\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2024-03-22 01:17:52,511] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2024-03-22 01:17:52,512] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2024-03-22 01:17:52,512] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"fp16_master_weights_and_grads\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 500, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1, \n",
      "        \"initial_scale_power\": 15\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"cpu_offload\": false\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10234808921813965 seconds\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py38_cu118/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.10249805450439453 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1021268367767334 seconds\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.0998222827911377 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1022348403930664 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10201883316040039 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10208845138549805 seconds\n",
      "STAGE:2024-03-22 01:17:54 27828:27828 ActivityProfilerController.cpp:294] Completed Stage: Warm Up\n",
      "[W CPUAllocator.cpp:231] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "STAGE:2024-03-22 01:17:54 27828:27828 ActivityProfilerController.cpp:300] Completed Stage: Collection\n",
      "STAGE:2024-03-22 01:17:55 27828:27828 output_json.cpp:417] Completed Stage: Post Processing\n",
      "[epoch 0, iterations   100] loss: 2.306 accuracy: 9.250000 %[epoch 0, iterations   100] loss: 2.300 accuracy: 10.750000 %[epoch 0, iterations   100] loss: 2.302 accuracy: 9.000000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   100] loss: 2.302 accuracy: 11.750000 %\n",
      "[2024-03-22 01:17:56,089] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 122\n",
      "[2024-03-22 01:17:56,089] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2024-03-22 01:17:56,089] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 122\n",
      "[2024-03-22 01:17:56,089] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 122\n",
      "[2024-03-22 01:17:56,090] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0\n",
      "[2024-03-22 01:17:56,090] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2024-03-22 01:17:56,090] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2024-03-22 01:17:56,089] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 122\n",
      "[2024-03-22 01:17:56,090] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 123\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 123\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 123\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 123\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2024-03-22 01:17:56,097] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2024-03-22 01:17:56,097] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
      "[epoch 0, iterations   200] loss: 2.237 accuracy: 12.750000 %[epoch 0, iterations   200] loss: 2.228 accuracy: 12.375000 %[epoch 0, iterations   200] loss: 2.231 accuracy: 11.000000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   200] loss: 2.233 accuracy: 12.375000 %\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 217\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 217\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 217\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 217\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:17:56,896] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:17:56,896] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations   300] loss: 2.181 accuracy: 13.750000 %[epoch 0, iterations   300] loss: 2.114 accuracy: 12.750000 %[epoch 0, iterations   300] loss: 2.189 accuracy: 12.666667 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   300] loss: 2.175 accuracy: 14.583333 %\n",
      "[epoch 0, iterations   400] loss: 2.130 accuracy: 14.750000 %\n",
      "[epoch 0, iterations   400] loss: 2.089 accuracy: 14.937500 %[epoch 0, iterations   400] loss: 2.127 accuracy: 13.000000 %\n",
      "\n",
      "[epoch 0, iterations   400] loss: 2.174 accuracy: 14.437500 %\n",
      "[epoch 0, iterations   500] loss: 2.139 accuracy: 14.850000 %\n",
      "[epoch 0, iterations   500] loss: 2.108 accuracy: 14.850000 %\n",
      "[epoch 0, iterations   500] loss: 2.037 accuracy: 13.900000 %\n",
      "[epoch 0, iterations   500] loss: 2.081 accuracy: 14.700000 %\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 592\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 592\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 592\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 592\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:00,069] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:00,069] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations   600] loss: 2.082 accuracy: 15.250000 %[epoch 0, iterations   600] loss: 2.049 accuracy: 15.625000 %[epoch 0, iterations   600] loss: 2.038 accuracy: 14.958333 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   600] loss: 2.010 accuracy: 15.416667 %\n",
      "[epoch 0, iterations   700] loss: 2.096 accuracy: 15.571429 %\n",
      "[epoch 0, iterations   700] loss: 2.039 accuracy: 15.285714 %[epoch 0, iterations   700] loss: 2.036 accuracy: 16.214286 %\n",
      "\n",
      "[epoch 0, iterations   700] loss: 2.070 accuracy: 16.321429 %\n",
      "[epoch 0, iterations   800] loss: 2.032 accuracy: 15.375000 %[epoch 0, iterations   800] loss: 1.982 accuracy: 16.812500 %\n",
      "\n",
      "[epoch 0, iterations   800] loss: 2.044 accuracy: 15.781250 %\n",
      "[epoch 0, iterations   800] loss: 2.013 accuracy: 16.593750 %\n",
      "[epoch 0, iterations   900] loss: 2.009 accuracy: 16.694444 %[epoch 0, iterations   900] loss: 2.033 accuracy: 15.583333 %[epoch 0, iterations   900] loss: 2.038 accuracy: 16.250000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   900] loss: 1.972 accuracy: 17.166667 %\n",
      "[epoch 0, iterations  1000] loss: 2.022 accuracy: 17.350000 %[epoch 0, iterations  1000] loss: 2.037 accuracy: 16.225000 %[epoch 0, iterations  1000] loss: 2.027 accuracy: 16.550000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1000] loss: 1.997 accuracy: 17.675000 %\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:04,248] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 0, iterations  1100] loss: 1.925 accuracy: 16.954545 %[epoch 0, iterations  1100] loss: 2.040 accuracy: 16.772727 %\n",
      "\n",
      "[epoch 0, iterations  1100] loss: 1.988 accuracy: 17.659091 %\n",
      "[epoch 0, iterations  1100] loss: 2.020 accuracy: 18.090909 %\n",
      "[epoch 0, iterations  1200] loss: 1.969 accuracy: 17.354167 %[epoch 0, iterations  1200] loss: 2.013 accuracy: 17.291667 %\n",
      "\n",
      "[epoch 0, iterations  1200] loss: 1.950 accuracy: 18.208333 %\n",
      "[epoch 0, iterations  1200] loss: 1.993 accuracy: 18.520833 %\n",
      "[epoch 0, iterations  1300] loss: 1.981 accuracy: 17.615385 %[epoch 0, iterations  1300] loss: 1.970 accuracy: 18.211538 %[epoch 0, iterations  1300] loss: 2.011 accuracy: 17.788462 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1300] loss: 1.980 accuracy: 19.057692 %\n",
      "[epoch 0, iterations  1400] loss: 1.998 accuracy: 18.642857 %[epoch 0, iterations  1400] loss: 1.946 accuracy: 18.071429 %[epoch 0, iterations  1400] loss: 2.013 accuracy: 17.928571 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1400] loss: 2.034 accuracy: 19.017857 %\n",
      "[epoch 0, iterations  1500] loss: 1.989 accuracy: 19.016667 %[epoch 0, iterations  1500] loss: 2.028 accuracy: 18.566667 %\n",
      "[epoch 0, iterations  1500] loss: 1.962 accuracy: 18.300000 %\n",
      "\n",
      "[epoch 0, iterations  1500] loss: 2.024 accuracy: 19.333333 %\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:08,405] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1597\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1597\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1597\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1597\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:08,438] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:08,438] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations  1600] loss: 1.912 accuracy: 19.500000 %[epoch 0, iterations  1600] loss: 1.996 accuracy: 18.578125 %[epoch 0, iterations  1600] loss: 1.952 accuracy: 18.765625 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1600] loss: 1.918 accuracy: 19.812500 %\n",
      "[epoch 0, iterations  1700] loss: 1.939 accuracy: 19.926471 %[epoch 0, iterations  1700] loss: 2.013 accuracy: 19.220588 %[epoch 0, iterations  1700] loss: 1.933 accuracy: 19.147059 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1700] loss: 1.956 accuracy: 20.014706 %\n",
      "[epoch 0, iterations  1800] loss: 1.919 accuracy: 19.402778 %\n",
      "[epoch 0, iterations  1800] loss: 1.894 accuracy: 19.763889 %\n",
      "[epoch 0, iterations  1800] loss: 1.923 accuracy: 20.180556 %\n",
      "[epoch 0, iterations  1800] loss: 1.900 accuracy: 20.527778 %\n",
      "[epoch 0, iterations  1900] loss: 1.885 accuracy: 20.565789 %[epoch 0, iterations  1900] loss: 1.864 accuracy: 20.078947 %\n",
      "\n",
      "[epoch 0, iterations  1900] loss: 1.855 accuracy: 20.144737 %\n",
      "[epoch 0, iterations  1900] loss: 1.891 accuracy: 20.907895 %\n",
      "[epoch 0, iterations  2000] loss: 1.862 accuracy: 20.712500 %[epoch 0, iterations  2000] loss: 1.851 accuracy: 20.612500 %[epoch 0, iterations  2000] loss: 1.910 accuracy: 20.862500 %\n",
      "\n",
      "\n",
      "[2024-03-22 01:18:11,747] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=5, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2024-03-22 01:18:11,748] [INFO] [timer.py:193:stop] 0/2000, SamplesPerSec=1974.6284307239898, MemAllocated=0.01GB, MaxMemAllocated=0.01GB\n",
      "[epoch 0, iterations  2000] loss: 1.882 accuracy: 21.287500 %\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2024-03-22 01:18:12,569] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[epoch 0, iterations  2100] loss: 1.876 accuracy: 21.095238 %\n",
      "[epoch 0, iterations  2100] loss: 1.843 accuracy: 21.035714 %[epoch 0, iterations  2100] loss: 1.822 accuracy: 21.130952 %\n",
      "\n",
      "[epoch 0, iterations  2100] loss: 1.933 accuracy: 21.511905 %\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2103\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2103\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2103\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2103\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:12,611] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2024-03-22 01:18:12,611] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2196\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2196\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2196\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2196\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:13,377] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:13,377] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  2200] loss: 1.860 accuracy: 21.522727 %[epoch 0, iterations  2200] loss: 1.879 accuracy: 21.500000 %\n",
      "\n",
      "[epoch 0, iterations  2200] loss: 1.911 accuracy: 21.500000 %\n",
      "[epoch 0, iterations  2200] loss: 1.860 accuracy: 21.886364 %\n",
      "[epoch 0, iterations  2300] loss: 1.776 accuracy: 22.065217 %\n",
      "[epoch 0, iterations  2300] loss: 1.875 accuracy: 21.717391 %[epoch 0, iterations  2300] loss: 1.852 accuracy: 21.891304 %\n",
      "\n",
      "[epoch 0, iterations  2300] loss: 1.790 accuracy: 22.293478 %\n",
      "[epoch 0, iterations  2400] loss: 1.828 accuracy: 22.427083 %[epoch 0, iterations  2400] loss: 1.774 accuracy: 22.125000 %[epoch 0, iterations  2400] loss: 1.813 accuracy: 22.343750 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2400] loss: 1.820 accuracy: 22.697917 %\n",
      "[epoch 0, iterations  2500] loss: 1.707 accuracy: 23.090000 %\n",
      "[epoch 0, iterations  2500] loss: 1.870 accuracy: 22.440000 %[epoch 0, iterations  2500] loss: 1.764 accuracy: 22.810000 %\n",
      "\n",
      "[epoch 0, iterations  2500] loss: 1.740 accuracy: 23.140000 %\n",
      "[epoch 0, iterations  2600] loss: 1.782 accuracy: 23.461538 %\n",
      "[epoch 0, iterations  2600] loss: 1.828 accuracy: 22.807692 %\n",
      "[epoch 0, iterations  2600] loss: 1.766 accuracy: 23.259615 %\n",
      "[epoch 0, iterations  2600] loss: 1.833 accuracy: 23.500000 %\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:17,727] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:17,728] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 0, iterations  2700] loss: 1.820 accuracy: 23.101852 %[epoch 0, iterations  2700] loss: 1.833 accuracy: 23.472222 %\n",
      "\n",
      "[epoch 0, iterations  2700] loss: 1.757 accuracy: 23.685185 %\n",
      "[epoch 0, iterations  2700] loss: 1.874 accuracy: 23.703704 %\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2707\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2707\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2707\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2707\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:17,815] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:17,815] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  2800] loss: 1.766 accuracy: 23.491071 %[epoch 0, iterations  2800] loss: 1.751 accuracy: 23.785714 %[epoch 0, iterations  2800] loss: 1.811 accuracy: 23.991071 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2800] loss: 1.816 accuracy: 23.982143 %\n",
      "[epoch 0, iterations  2900] loss: 1.736 accuracy: 24.155172 %[epoch 0, iterations  2900] loss: 1.756 accuracy: 23.793103 %[epoch 0, iterations  2900] loss: 1.798 accuracy: 24.353448 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2900] loss: 1.858 accuracy: 24.370690 %\n",
      "[epoch 0, iterations  3000] loss: 1.702 accuracy: 24.491667 %[epoch 0, iterations  3000] loss: 1.688 accuracy: 24.258333 %\n",
      "\n",
      "[epoch 0, iterations  3000] loss: 1.718 accuracy: 24.758333 %\n",
      "[epoch 0, iterations  3000] loss: 1.809 accuracy: 24.650000 %\n",
      "[epoch 0, iterations  3100] loss: 1.800 accuracy: 25.129032 %\n",
      "[epoch 0, iterations  3100] loss: 1.770 accuracy: 24.822581 %\n",
      "[epoch 0, iterations  3100] loss: 1.830 accuracy: 24.467742 %\n",
      "[epoch 0, iterations  3100] loss: 1.786 accuracy: 24.991935 %\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:23,011] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3220\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3220\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3220\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3220\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:23,116] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:23,116] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations   100] loss: 1.747 accuracy: 33.500000 %\n",
      "[epoch 1, iterations   100] loss: 1.674 accuracy: 38.000000 %\n",
      "[epoch 1, iterations   100] loss: 1.719 accuracy: 37.500000 %\n",
      "[epoch 1, iterations   100] loss: 1.739 accuracy: 34.000000 %\n",
      "[epoch 1, iterations   200] loss: 1.626 accuracy: 35.500000 %\n",
      "[epoch 1, iterations   200] loss: 1.750 accuracy: 34.750000 %\n",
      "[epoch 1, iterations   200] loss: 1.722 accuracy: 37.250000 %\n",
      "[epoch 1, iterations   200] loss: 1.797 accuracy: 33.250000 %\n",
      "[epoch 1, iterations   300] loss: 1.759 accuracy: 35.083333 %\n",
      "[epoch 1, iterations   300] loss: 1.713 accuracy: 36.166667 %\n",
      "[epoch 1, iterations   300] loss: 1.760 accuracy: 33.750000 %\n",
      "[epoch 1, iterations   300] loss: 1.702 accuracy: 34.000000 %\n",
      "[epoch 1, iterations   400] loss: 1.758 accuracy: 34.812500 %[epoch 1, iterations   400] loss: 1.680 accuracy: 34.750000 %[epoch 1, iterations   400] loss: 1.630 accuracy: 36.625000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   400] loss: 1.709 accuracy: 34.000000 %\n",
      "[epoch 1, iterations   500] loss: 1.737 accuracy: 35.100000 %[epoch 1, iterations   500] loss: 1.604 accuracy: 35.700000 %\n",
      "\n",
      "[epoch 1, iterations   500] loss: 1.789 accuracy: 35.850000 %\n",
      "[epoch 1, iterations   500] loss: 1.670 accuracy: 34.600000 %\n",
      "[2024-03-22 01:18:27,522] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:27,522] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:27,523] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:27,522] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:27,523] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:27,522] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:27,523] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:27,523] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 1, iterations   600] loss: 1.669 accuracy: 35.375000 %[epoch 1, iterations   600] loss: 1.703 accuracy: 35.916667 %\n",
      "[epoch 1, iterations   600] loss: 1.702 accuracy: 35.791667 %\n",
      "\n",
      "[epoch 1, iterations   600] loss: 1.627 accuracy: 35.666667 %\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3735\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3735\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3735\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3735\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:27,645] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:27,645] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations   700] loss: 1.650 accuracy: 35.464286 %[epoch 1, iterations   700] loss: 1.777 accuracy: 35.357143 %\n",
      "\n",
      "[epoch 1, iterations   700] loss: 1.621 accuracy: 36.857143 %\n",
      "[epoch 1, iterations   700] loss: 1.725 accuracy: 35.500000 %\n",
      "[epoch 1, iterations   800] loss: 1.753 accuracy: 35.968750 %[epoch 1, iterations   800] loss: 1.738 accuracy: 35.625000 %[epoch 1, iterations   800] loss: 1.558 accuracy: 36.343750 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   800] loss: 1.688 accuracy: 35.625000 %\n",
      "[2024-03-22 01:18:29,938] [INFO] [logging.py:69:log_dist] [Rank 0] step=4000, skipped=10, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2024-03-22 01:18:29,939] [INFO] [timer.py:193:stop] 0/4000, SamplesPerSec=1898.6404019273682, MemAllocated=0.01GB, MaxMemAllocated=0.01GB\n",
      "[epoch 1, iterations   900] loss: 1.651 accuracy: 36.027778 %[epoch 1, iterations   900] loss: 1.721 accuracy: 36.000000 %\n",
      "\n",
      "[epoch 1, iterations   900] loss: 1.697 accuracy: 35.527778 %[epoch 1, iterations   900] loss: 1.630 accuracy: 36.416667 %\n",
      "\n",
      "[epoch 1, iterations  1000] loss: 1.702 accuracy: 36.450000 %[epoch 1, iterations  1000] loss: 1.634 accuracy: 36.175000 %\n",
      "[epoch 1, iterations  1000] loss: 1.622 accuracy: 36.250000 %\n",
      "\n",
      "[epoch 1, iterations  1000] loss: 1.626 accuracy: 36.250000 %\n",
      "[epoch 1, iterations  1100] loss: 1.647 accuracy: 36.750000 %\n",
      "[epoch 1, iterations  1100] loss: 1.657 accuracy: 36.477273 %[epoch 1, iterations  1100] loss: 1.648 accuracy: 36.636364 %\n",
      "\n",
      "[epoch 1, iterations  1100] loss: 1.645 accuracy: 36.272727 %\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:31,976] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4272\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4272\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4272\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4272\n",
      "[2024-03-22 01:18:32,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:32,286] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:32,286] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:32,286] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  1200] loss: 1.602 accuracy: 37.062500 %[epoch 1, iterations  1200] loss: 1.654 accuracy: 36.958333 %\n",
      "\n",
      "[epoch 1, iterations  1200] loss: 1.781 accuracy: 36.437500 %\n",
      "[epoch 1, iterations  1200] loss: 1.569 accuracy: 36.562500 %\n",
      "[epoch 1, iterations  1300] loss: 1.618 accuracy: 36.923077 %[epoch 1, iterations  1300] loss: 1.798 accuracy: 36.519231 %\n",
      "\n",
      "[epoch 1, iterations  1300] loss: 1.670 accuracy: 36.615385 %\n",
      "[epoch 1, iterations  1300] loss: 1.663 accuracy: 36.557692 %\n",
      "[epoch 1, iterations  1400] loss: 1.678 accuracy: 36.928571 %[epoch 1, iterations  1400] loss: 1.755 accuracy: 36.321429 %\n",
      "\n",
      "[epoch 1, iterations  1400] loss: 1.651 accuracy: 36.535714 %\n",
      "[epoch 1, iterations  1400] loss: 1.667 accuracy: 36.767857 %\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4535\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4535\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4535\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4535\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:34,549] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:34,549] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n",
      "[epoch 1, iterations  1500] loss: 1.705 accuracy: 36.966667 %[epoch 1, iterations  1500] loss: 1.665 accuracy: 36.800000 %\n",
      "\n",
      "[epoch 1, iterations  1500] loss: 1.658 accuracy: 36.666667 %\n",
      "[epoch 1, iterations  1500] loss: 1.704 accuracy: 36.750000 %\n",
      "[epoch 1, iterations  1600] loss: 1.613 accuracy: 37.109375 %[epoch 1, iterations  1600] loss: 1.738 accuracy: 36.390625 %\n",
      "[epoch 1, iterations  1600] loss: 1.651 accuracy: 36.875000 %\n",
      "\n",
      "[epoch 1, iterations  1600] loss: 1.667 accuracy: 36.828125 %\n",
      "[epoch 1, iterations  1700] loss: 1.595 accuracy: 37.308824 %[epoch 1, iterations  1700] loss: 1.531 accuracy: 37.014706 %\n",
      "[epoch 1, iterations  1700] loss: 1.695 accuracy: 36.926471 %\n",
      "\n",
      "[epoch 1, iterations  1700] loss: 1.729 accuracy: 36.911765 %\n",
      "[epoch 1, iterations  1800] loss: 1.675 accuracy: 37.361111 %[epoch 1, iterations  1800] loss: 1.690 accuracy: 37.027778 %\n",
      "[epoch 1, iterations  1800] loss: 1.630 accuracy: 37.222222 %\n",
      "\n",
      "[epoch 1, iterations  1800] loss: 1.592 accuracy: 37.222222 %\n",
      "[epoch 1, iterations  1900] loss: 1.570 accuracy: 37.644737 %[epoch 1, iterations  1900] loss: 1.599 accuracy: 37.236842 %[epoch 1, iterations  1900] loss: 1.690 accuracy: 37.144737 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1900] loss: 1.627 accuracy: 37.289474 %\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:38,894] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[epoch 1, iterations  2000] loss: 1.616 accuracy: 37.550000 %[epoch 1, iterations  2000] loss: 1.569 accuracy: 37.350000 %\n",
      "\n",
      "[epoch 1, iterations  2000] loss: 1.624 accuracy: 37.250000 %\n",
      "[epoch 1, iterations  2000] loss: 1.729 accuracy: 37.200000 %\n",
      "[epoch 1, iterations  2100] loss: 1.561 accuracy: 37.666667 %[epoch 1, iterations  2100] loss: 1.576 accuracy: 37.547619 %[epoch 1, iterations  2100] loss: 1.642 accuracy: 37.428571 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2100] loss: 1.750 accuracy: 37.178571 %\n",
      "[epoch 1, iterations  2200] loss: 1.597 accuracy: 37.965909 %[epoch 1, iterations  2200] loss: 1.674 accuracy: 37.681818 %\n",
      "[epoch 1, iterations  2200] loss: 1.657 accuracy: 37.613636 %\n",
      "\n",
      "[epoch 1, iterations  2200] loss: 1.543 accuracy: 37.363636 %\n",
      "[epoch 1, iterations  2300] loss: 1.567 accuracy: 38.130435 %[epoch 1, iterations  2300] loss: 1.646 accuracy: 37.673913 %\n",
      "[epoch 1, iterations  2300] loss: 1.689 accuracy: 37.630435 %\n",
      "\n",
      "[epoch 1, iterations  2300] loss: 1.522 accuracy: 37.608696 %\n",
      "[epoch 1, iterations  2400] loss: 1.543 accuracy: 38.312500 %[epoch 1, iterations  2400] loss: 1.593 accuracy: 37.729167 %\n",
      "\n",
      "[epoch 1, iterations  2400] loss: 1.599 accuracy: 37.614583 %\n",
      "[epoch 1, iterations  2400] loss: 1.615 accuracy: 37.677083 %\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:43,232] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5538\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5538\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5538\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5538\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:43,249] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2024-03-22 01:18:43,249] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  2500] loss: 1.609 accuracy: 38.380000 %[epoch 1, iterations  2500] loss: 1.600 accuracy: 37.900000 %\n",
      "\n",
      "[epoch 1, iterations  2500] loss: 1.538 accuracy: 37.800000 %\n",
      "[epoch 1, iterations  2500] loss: 1.498 accuracy: 37.940000 %\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5695\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5695\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5695\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5695\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:44,612] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2024-03-22 01:18:44,613] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n",
      "[epoch 1, iterations  2600] loss: 1.620 accuracy: 38.471154 %[epoch 1, iterations  2600] loss: 1.565 accuracy: 38.019231 %[epoch 1, iterations  2600] loss: 1.587 accuracy: 37.855769 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2600] loss: 1.662 accuracy: 38.009615 %\n",
      "[epoch 1, iterations  2700] loss: 1.591 accuracy: 38.018519 %[epoch 1, iterations  2700] loss: 1.620 accuracy: 37.777778 %[epoch 1, iterations  2700] loss: 1.643 accuracy: 38.546296 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2700] loss: 1.584 accuracy: 38.083333 %\n",
      "[epoch 1, iterations  2800] loss: 1.548 accuracy: 38.705357 %[epoch 1, iterations  2800] loss: 1.608 accuracy: 38.133929 %\n",
      "\n",
      "[epoch 1, iterations  2800] loss: 1.579 accuracy: 37.821429 %\n",
      "[epoch 1, iterations  2800] loss: 1.599 accuracy: 38.098214 %\n",
      "[2024-03-22 01:18:47,263] [INFO] [logging.py:69:log_dist] [Rank 0] step=6000, skipped=14, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2024-03-22 01:18:47,264] [INFO] [timer.py:193:stop] 0/6000, SamplesPerSec=1903.576680762575, MemAllocated=0.01GB, MaxMemAllocated=0.01GB\n",
      "[epoch 1, iterations  2900] loss: 1.633 accuracy: 38.232759 %[epoch 1, iterations  2900] loss: 1.528 accuracy: 38.758621 %[epoch 1, iterations  2900] loss: 1.586 accuracy: 37.905172 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2900] loss: 1.725 accuracy: 38.086207 %\n",
      "[epoch 1, iterations  3000] loss: 1.580 accuracy: 38.291667 %[epoch 1, iterations  3000] loss: 1.575 accuracy: 38.058333 %[epoch 1, iterations  3000] loss: 1.512 accuracy: 38.908333 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  3000] loss: 1.703 accuracy: 38.000000 %\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2024-03-22 01:18:48,989] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[epoch 1, iterations  3100] loss: 1.521 accuracy: 38.564516 %[epoch 1, iterations  3100] loss: 1.637 accuracy: 38.975806 %[epoch 1, iterations  3100] loss: 1.617 accuracy: 38.153226 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  3100] loss: 1.537 accuracy: 38.241935 %\n",
      "Training Done\n",
      "Training Done\n",
      "Training Done\n",
      "Training Done\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/usr/local/lib/python3.8/dist-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:1076: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 6.353271484375 at index (1, 8) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 160.492660011744 at index (3, 6) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:1076: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 7.6546630859375 at index (0, 7) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 191.56371109806224 at index (2, 6) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:1076: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 6.0936279296875 at index (2, 1) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 173.82560187903698 at index (2, 6) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/jit/_trace.py:1076: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the repeated trace. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 2.4178466796875 at index (2, 1) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 38.30945390487375 at index (2, 6) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "Accuracy of the network on the 10000 test images: 41.320000 %\n",
      "Accuracy of plane : 44.400000 %\n",
      "Accuracy of   car : 49.700000 %\n",
      "Accuracy of  bird : 23.300000 %\n",
      "Accuracy of   cat : 33.300000 %\n",
      "Accuracy of  deer : 28.300000 %\n",
      "Accuracy of   dog : 41.200000 %\n",
      "Accuracy of  frog : 47.400000 %\n",
      "Accuracy of horse : 56.700000 %\n",
      "Accuracy of  ship : 62.100000 %\n",
      "Accuracy of truck : 26.800000 %\n",
      "Evaluation Done\n",
      "Accuracy of the network on the 10000 test images: 41.950000 %\n",
      "Accuracy of plane : 46.500000 %\n",
      "Accuracy of   car : 51.600000 %\n",
      "Accuracy of  bird : 27.300000 %\n",
      "Accuracy of   cat : 29.900000 %\n",
      "Accuracy of  deer : 29.300000 %\n",
      "Accuracy of   dog : 39.500000 %\n",
      "Accuracy of  frog : 47.400000 %\n",
      "Accuracy of horse : 54.600000 %\n",
      "Accuracy of  ship : 65.400000 %\n",
      "Accuracy of truck : 28.000000 %\n",
      "Evaluation Done\n",
      "Accuracy of the network on the 10000 test images: 41.880000 %\n",
      "Accuracy of plane : 44.400000 %\n",
      "Accuracy of   car : 49.500000 %\n",
      "Accuracy of  bird : 27.000000 %\n",
      "Accuracy of   cat : 33.500000 %\n",
      "Accuracy of  deer : 28.400000 %\n",
      "Accuracy of   dog : 41.300000 %\n",
      "Accuracy of  frog : 48.400000 %\n",
      "Accuracy of horse : 55.500000 %\n",
      "Accuracy of  ship : 62.700000 %\n",
      "Accuracy of truck : 28.100000 %\n",
      "Evaluation Done\n",
      "[2024-03-22 01:19:00,631] [INFO] [launch.py:210:main] Process 27830 exits successfully.\n",
      "[2024-03-22 01:19:00,631] [INFO] [launch.py:210:main] Process 27831 exits successfully.\n",
      "[2024-03-22 01:19:00,631] [INFO] [launch.py:210:main] Process 27829 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed_MOE.py  \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --moe \\\n",
    "    --ep-world-size 4 \\\n",
    "    --num-experts-per-layer 8 \\\n",
    "    --top-k 1 \\\n",
    "    --noisy-gate-policy 'RSample' \\\n",
    "    --moe-param-group \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0_MOE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepspeed_MOE.png\" width=\"950\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "The next lab will focus on deploying large neural networks.\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
