{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Multi-GPU Training Strategies\n",
    "\n",
    "In this notebook, we will introduce the basic knowledge of distributed training strategies and experiments with [NeMo Framework](https://github.com/NVIDIA/NeMo/), NVIDIA conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the mechanisms behind distributed training strategies\n",
    "* Run a simple distributed training using NeMo Framework scripts on 1 Node with data parallel distribution\n",
    "* Understand the basic outputs of NeMo Framework logs\n",
    "\n",
    "**[2.1 Introduction to Distributed Training Strategies](#2.1-Introduction-to-Distributed-Training-Strategies)<br>**\n",
    "**[2.2 Single GPU Training Execution of NeMo GPT Pretraining](#2.2-Single-GPU-Training-Execution-of-NeMo-GPT-Pretraining)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.1 Check The GPT pretraining Script](#221-check-the-gpt-pretraining-script)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.2 Run the GPT pretraining Script](#222-run-the-gpt-pretraining-script)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.2.3 Understanding NeMo Framework Execution Logs](#223-understanding-nemo-framework-execution-logs)<br>\n",
    "**[2.3 Multi-GPU Training Execution of NeMo GPT Pretraining](#2.3-Multi-GPU-Training-Execution-of-NeMo-GPT-Pretraining)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.1 Exercise: NeMo GPT pretraining execution on 2 GPUs](#2.3.1-Exercise:-NeMo-GPT-pretraining-execution-on-2-GPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.2 Understanding Multi-GPU NeMo Framework Execution Logs](#223-understanding-nemo-framework-execution-logs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[2.3.3 Model Distribution Considerations ](#233-model-distribution-considerations)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel root user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.1 Introduction to Distributed Training Strategies\n",
    "In distributed training mode, the goal is to split the training process across multiple machines. The most commonly used distribution strategies are **data** and **model** parallelism.\n",
    "\n",
    "\n",
    "## Data Distribution \n",
    "\n",
    "Neural Networks are usually trained using [Stochastic Gradient Descent algorithms](https://developer.nvidia.com/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/) consisting of splitting the dataset into batches processed sequentially. At the forward step, the feature maps are computed while at the backward pass, gradients are computed and averaged to determine the parameter updates. Finally, the next batch of data is processed once the model's parameters are updated.\n",
    "\n",
    "<img src=\"images/data_parallel.png\" width=\"600\"/>\n",
    "\n",
    "In data parallelism mode, the data is split across multiple machines, each will be processed by a copy of the same neural network hosted by each machine. Parameter updates are averaged from all machines and model updates are reflected on all copies.\n",
    "\n",
    "Since, with more processors (or alternatively higher data parallelism), the time duration of an epoch (i.e. entire dataset) reduces, this has the effect of speeding up training. Also, because the updated gradients have effectively seen a larger number of samples (due to increased global batch size), this has a positive effect on the time to convergence. The time taken per batch is still the same, with the added cost of gradient exchange communication. \n",
    "\n",
    "There are different strategies for implementing the exchange of gradients: \n",
    "- **Centralized** way, where a server machine is responsible for distributing data chunks, accumulating gradients, and updating model parameters. \n",
    "- **Decentralized** way, where each worker sends and collects gradients from others to aggregate and update the model’s parameters locally. In addition, the workers can deliver the computation at different speeds. So, model parameters can be updated in a synchronous way based on the worker's synchronization points. We can also use a relaxed strategy, allowing workers to operate with outdated parameters. This strategy may introduce inconsistency during the training.\n",
    "\n",
    "Several libraries offer data parallelism implementations such as [Horovod](https://github.com/horovod/horovod) which is compatible with several Deep Learning Frameworks such as TensorFlow, Keras, PyTorch, Apache MXNet. [NVIDIA APEX](https://nvidia.github.io/apex/) is a Pytorch extension library that offers utilities to streamline distributed training and Mixed Precision.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Model Distribution Strategies\n",
    "\n",
    "Model parallelism is the process of splitting a model’s parameters across multiple machines. This allows training bigger models that do not fit into 1 GPU, with the cost of additional communications due to feature maps exchange. \n",
    "\n",
    "We can distinguish 2 types of model distribution:\n",
    "\n",
    "### Pipeline Parallelism\n",
    "\n",
    "\n",
    "<img src=\"images/pipeline_parallel1.png\" width=\"600\"/>\n",
    "\n",
    "Pipeline Parallelism is the process of cutting a model sequentially into pieces and assigning each part to a specific worker. For instance layers 1,2 on device_1 and 3,4 on device_2, and so on. \n",
    "\n",
    "There are different pipelining strategies such as the Micro-batch pipeline parallelism [GPipe](https://arxiv.org/pdf/1811.06965.pdf), which is an optimized implementation of model pipelining to minimize the time machines wait for their peers to communicate their outputs. It consists in partitioning data chunks into micro batches enabling different machines to process different micro batches simultaneously.\n",
    "\n",
    "![title](images/pipeline_parallel.png)\n",
    "\n",
    "Instead of a single sequential set of layers per device, the Interleaved pipeline parallelism assigns multiple pipeline stages per device, each with less computation. \n",
    "For instance, layers 1,2 and 9,10 on device_1, layers 3,4 and 11,12 on device_2, and so on. \n",
    "\n",
    "\n",
    "### Tensor Parallelism\n",
    "\n",
    "<img src=\"images/tensor_parallel1.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "Tensor Parallelism is the process of dividing matrix operations across workers. \n",
    "\n",
    "[NeMo Framework](https://github.com/NVIDIA/NeMo/) is NVIDIA's open-source library for efficient training of transformer-based networks and multi-node pre-training of transformer-based models such as GPT, BERT, and T5 using mixed precision with a focus on conversational AI.\n",
    "\n",
    "<img src=\"images/tensor_parallel.png\" width=\"250\"/>\n",
    "\n",
    "NeMo Framework is built on top of PyTorch and it integrates data, pipeline and tensor parallelism for pre-training of GPT and BERT transformers architectures using mixed precision. In this lab, we will be using implemented distribution strategies provided by NeMo Framework. In particular, we will focus on 1 Node execution on 1 or 2 devices and in the second case, we will use data, tensor and pipeline parallelism. Several examples can be found in the [NeMo Framework repository](https://github.com/NVIDIA/NeMo/tree/main/examples). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2.2 Single GPU Training Execution of NeMo GPT Pretraining \n",
    "\n",
    "Let's first get familiarized with a simple NeMo GPT execution script. \n",
    "\n",
    "For distributed training mode, it's sufficient to use Python. There's no need to use the PyTorch distributed launcher ([torchrun](https://pytorch.org/docs/stable/elastic/run.html) or [torch.distributed](https://pytorch.org/tutorials/beginner/dist_overview.html)) even for multi-node jobs, as NeMo Framework handles multi-node communication automatically.\n",
    "\n",
    "The resources are configured with the arguments `trainer.num_nodes` and `trainer.devices` which specify respectively the number of nodes and GPUs per node to use.\n",
    "\n",
    "In this section, we will run a simple NeMo GPT Pretraining Execution on 1 GPU by running the NeMo [megatron_gpt_pretraining.py](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/megatron_gpt_pretraining.py) script. We will use an [example training config](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/conf/megatron_gpt_config.yaml) for GPT models as foundation. We will overwrite some arguments using [Hydra](https://hydra.cc/docs/intro/), which allows to perform on the fly config edits from the command line. You can find some of the config parameters on the following image:\n",
    "\n",
    "<img src=\"images/nemo_run.png\" width=\"600\"/>\n",
    "\n",
    "Distributed strategies are configured with the arguments `model.tensor_model_parallel_size` and `model.pipeline_model_parallel_size`\n",
    "Learn more about the distributed strategies arguments in the [NeMo documentation](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html).\n",
    "\n",
    "We have prepared the script [pretrain_gpt_1GPU.sh](./code/pretrain_gpt_1GPU.sh) that will run GPT pretraining on only 1 GPU (with no distribution strategy applied).\n",
    "\n",
    "This script assumes that the compute resources are already allocated. Thus, for the execution, we will need to first allocate the required GPU by connecting to a worker node in an interactive session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1 Check the GPT Pretraining Script\n",
    "\n",
    "Let's have a look at the script before allocating the resources and executing it. \n",
    "\n",
    "Notice the model architecture and training arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# Distributed training args\n",
      "NNODES=1\n",
      "GPUS_PER_NODE=1\n",
      "TP_SIZE=1\n",
      "PP_SIZE=1\n",
      "\n",
      "# Distributed training \n",
      "MICRO_BATCH_SIZE=16\n",
      "GLOBAL_BATCH_SIZE=16\n",
      "\n",
      "# Model architecture \n",
      "NLAYERS=12\n",
      "NHIDDEN=768\n",
      "NHEADS=32\n",
      "SEQ_LEN=1024\n",
      "\n",
      "# Data Paths\n",
      "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
      "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
      "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
      "\n",
      "OUTPUT_PATH=/dli/nemo\n",
      "LOGS_PATH=/dli/nemo/logs\n",
      "NAME=\"1Node1GPU\"\n",
      "\n",
      "\n",
      "OPTIMIZER_ARGS=\" \\\n",
      "            model.optim.name=fused_adam \\\n",
      "            model.optim.betas=[0.9,0.95] \\\n",
      "            model.optim.lr=6e-5 \\\n",
      "            model.optim.sched.min_lr=6e-6 \\\n",
      "            model.optim.sched.name=CosineAnnealing \\\n",
      "            +model.optim.sched.max_steps=800 \\\n",
      "            model.optim.sched.warmup_steps=80 \\\n",
      "            model.optim.weight_decay=1e-1 \\\n",
      "        \"\n",
      "        \n",
      "TRAINER_ARGS=\" \\\n",
      "            trainer.gradient_clip_val=1.0 \\\n",
      "            trainer.precision=32 \\\n",
      "            trainer.devices=$GPUS_PER_NODE \\\n",
      "            trainer.num_nodes=$NNODES \\\n",
      "            trainer.max_steps=100 \\\n",
      "            trainer.enable_model_summary=true \\\n",
      "            trainer.log_every_n_steps=10 \\\n",
      "            trainer.val_check_interval=20 \\\n",
      "            trainer.limit_val_batches=10 \\\n",
      "        \"\n",
      "\n",
      "GPT_ARGS=\" \\\n",
      "            model.num_layers=$NLAYERS \\\n",
      "            model.hidden_size=$NHIDDEN \\\n",
      "            model.num_attention_heads=$NHEADS \\\n",
      "            model.encoder_seq_length=$SEQ_LEN \\\n",
      "            model.data.seq_length=$SEQ_LEN \\\n",
      "            model.max_position_embeddings=$SEQ_LEN \\\n",
      "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
      "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
      "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
      "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
      "            model.init_method_std=0.006 \\\n",
      "            $OPTIMIZER_ARGS \\\n",
      "        \"\n",
      "\n",
      "OUTPUT_ARGS=\" \\\n",
      "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
      "            exp_manager.resume_if_exists=false \\\n",
      "            exp_manager.name=$NAME \\\n",
      "        \"\n",
      "        \n",
      "PARALLEL_ARGS=\" \\\n",
      "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
      "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
      "        \"\n",
      "        \n",
      "\n",
      "export CMD=\" \\\n",
      "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
      "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
      "            --config-name=megatron_gpt_config.yaml \\\n",
      "            $TRAINER_ARGS \\\n",
      "            $PARALLEL_ARGS \\\n",
      "            $GPT_ARGS \\\n",
      "            $OUTPUT_ARGS \\\n",
      "            model.data.data_prefix=$DATA_PATH \\\n",
      "            model.data.data_impl=mmap \\\n",
      "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
      "        \"\n",
      "\n",
      "bash -c '$LAUNCHER $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt\n"
     ]
    }
   ],
   "source": [
    "# Have a look at the NeMo GPT pretraining execution on 1 GPU script\n",
    "!cat /dli/code/pretrain_gpt_1GPU.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2.2.2 Run the GPT Pretraining Script\n",
    "\n",
    "Now, let's run the pretrain_gpt_1GPU.sh script in an interactive session. To do so, follow the 3 steps:\n",
    "1. Launch a terminal session\n",
    "2. Run an interactive session by executing `srun -N 1 --pty /bin/bash`\n",
    "3. Run the NeMo GPT-3 pretraining on 1 GPU by executing `bash ./code/pretrain_gpt_1GPU.sh`\n",
    "\n",
    "\n",
    "<img src=\"images/interactive_launch0.png\" width=\"1050\"/>\n",
    "\n",
    "Run the following cell to generate a link to open a terminal session and the instructions to run interactive session. Then, submit a GPT pretraining job on 1 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>\n",
       "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
       "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
       "   Step 3: Run the NeMo gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_1GPU.sh</font>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 3: Run the NeMo gpt3 pretraining on 1 GPU: <font color=\"green\">bash ./code/pretrain_gpt_1GPU.sh</font>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the GPT pretraining on 1 GPU is running. We can check the SLURM queue by running this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                10  slurmpar     bash     root  R       0:31      1 slurmnode1\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the GPUs using the `nvidia-smi` command. We should see only GPU 0 utilized as shown in the figure bellow. Note that GPU types and memory can be different, depending on configuration of your course environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1N_1gpu_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 21:26:10 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             327W / 300W |  79613MiB / 81920MiB |     93%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              55W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU utilization on the master node after NeMo starts running\n",
    "!sleep 60s\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3  Understanding NeMo Framework Execution Logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the performance of the GPT pretraining, we can check the generated [log file](./nemo/logs/log_1GPU.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  59%|██████████████████▉             | 89/150 [01:00<00:41,  1.47it/s, loss=9.58, v_num=, reduced_train_loss=9.300, global_step=59.00, consumed_samples=944.0, val_loss=9.820]\u001b[A\n",
      "Epoch 0:  60%|███████████████████▏            | 90/150 [01:00<00:40,  1.48it/s, loss=9.58, v_num=, reduced_train_loss=9.300, global_step=59.00, consumed_samples=944.0, val_loss=9.260]\u001b[A\n",
      "                                                                                                                                                                                       \u001b[AEpoch 0, global step 60: 'val_loss' reached 9.26304 (best 9.26304), saving model to '/dli/nemo/1Node1GPU/checkpoints/megatron_gpt--val_loss=9.26-step=60-consumed_samples=944.0.ckpt' as top 10\n"
     ]
    }
   ],
   "source": [
    "# Check the NeMo GPT3 pretraining logs.\n",
    "!cat /dli/nemo/logs/1Node1GPU.txt | tail -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract, the outputs should be similar to:\n",
    "\n",
    "```\n",
    "Epoch 0: 100%|█| 150/150 [01:44<00:00,  1.43it/s, loss=8.52, v_num=, reduced_train_loss=8.56\n",
    "```   \n",
    "\n",
    "In this example, notice the training speed of 1.43 it/s, which is equal to 22.88 samples/s with a batch size of 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Great! Before moving on, let's release some disk space by deleting the unnecessary checkpoints and logs generated by the previous execution and cancel the remaining interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints and logs\n",
    "!rm -rf /dli/nemo/1Node1GPU/checkpoints\n",
    "!scancel -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 2.3 Multi-GPU Training Execution of NeMo GPT Pretraining\n",
    "\n",
    "Let's now execute the same previous training job while using the 2 GPUs available in the interactive session. \n",
    "\n",
    "For that, we need to set the number of devices per node to `trainer.devices=2`. \n",
    "\n",
    "The first distribution strategy we will experiment with is the data parallel distribution strategy, which is executed by default with NeMo when several resources are available.\n",
    "             \n",
    "In the previous execution on one single GPU, the batch size processed by the GPU was 16 (set by `model.micro_batch_size`) which also corresponds to the global batch size (set by `model.global_batch_size`). \n",
    "\n",
    "\n",
    "## 2.3.1 Exercise: NeMo GPT pretraining execution on 2 GPUs\n",
    "Let's configure the new NeMo GPT pretraining execution on 2 GPUs using data parallel distribution by modifying the `#FIXME` in the following cell. \n",
    "\n",
    "To use 2 GPUs, we can keep the micro batch size per GPUs set to 16 and thus double the global batch size to 32. If you get stuck, feel free to look at the [solution](./solutions/ex2.3.ipynb).\n",
    "\n",
    "Please notice that we will change the logfile name for each run (*1Node2GPUS.txt* in the following example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/pretrain_gpt_2GPU.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/pretrain_gpt_2GPU.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Distributed training args\n",
    "NNODES=1\n",
    "GPUS_PER_NODE=2         # <--- CHANGE HERE\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Distributed training \n",
    "MICRO_BATCH_SIZE=16\n",
    "GLOBAL_BATCH_SIZE=32    # <--- CHANGE HERE\n",
    "\n",
    "# Model architecture \n",
    "NLAYERS=12\n",
    "NHIDDEN=768\n",
    "NHEADS=32\n",
    "SEQ_LEN=1024\n",
    "\n",
    "# Data Paths\n",
    "VOCAB_FILE=/dli/data/GPT-2_assets/gpt2-vocab.json\n",
    "MERGE_FILE=/dli/data/GPT-2_assets/gpt2-merges.txt\n",
    "DATA_PATH=[1.0,/dli/data/GPT-2_assets/my-gpt2_text_document]\n",
    "\n",
    "OUTPUT_PATH=/dli/nemo\n",
    "LOGS_PATH=/dli/nemo/logs\n",
    "NAME=\"1Node2GPUS\"      \n",
    "\n",
    "\n",
    "OPTIMIZER_ARGS=\" \\\n",
    "            model.optim.name=fused_adam \\\n",
    "            model.optim.betas=[0.9,0.95] \\\n",
    "            model.optim.lr=6e-5 \\\n",
    "            model.optim.sched.min_lr=6e-6 \\\n",
    "            model.optim.sched.name=CosineAnnealing \\\n",
    "            +model.optim.sched.max_steps=800 \\\n",
    "            model.optim.sched.warmup_steps=80 \\\n",
    "            model.optim.weight_decay=1e-1 \\\n",
    "        \"\n",
    "        \n",
    "TRAINER_ARGS=\" \\\n",
    "            trainer.gradient_clip_val=1.0 \\\n",
    "            trainer.precision=32 \\\n",
    "            trainer.devices=$GPUS_PER_NODE \\\n",
    "            trainer.num_nodes=$NNODES \\\n",
    "            trainer.max_steps=100 \\\n",
    "            trainer.enable_model_summary=true \\\n",
    "            trainer.log_every_n_steps=10 \\\n",
    "            trainer.val_check_interval=20 \\\n",
    "            trainer.limit_val_batches=10 \\\n",
    "        \"\n",
    "\n",
    "GPT_ARGS=\" \\\n",
    "            model.num_layers=$NLAYERS \\\n",
    "            model.hidden_size=$NHIDDEN \\\n",
    "            model.num_attention_heads=$NHEADS \\\n",
    "            model.encoder_seq_length=$SEQ_LEN \\\n",
    "            model.data.seq_length=$SEQ_LEN \\\n",
    "            model.max_position_embeddings=$SEQ_LEN \\\n",
    "            model.micro_batch_size=$MICRO_BATCH_SIZE \\\n",
    "            model.global_batch_size=$GLOBAL_BATCH_SIZE \\\n",
    "            model.tokenizer.vocab_file=$VOCAB_FILE \\\n",
    "            model.tokenizer.merge_file=$MERGE_FILE \\\n",
    "            model.init_method_std=0.006 \\\n",
    "            $OPTIMIZER_ARGS \\\n",
    "        \"\n",
    "\n",
    "OUTPUT_ARGS=\" \\\n",
    "            exp_manager.explicit_log_dir=$OUTPUT_PATH/$NAME \\\n",
    "            exp_manager.resume_if_exists=false \\\n",
    "            exp_manager.name=$NAME \\\n",
    "        \"\n",
    "\n",
    "PARALLEL_ARGS=\" \\\n",
    "            model.tensor_model_parallel_size=$TP_SIZE \\\n",
    "            model.pipeline_model_parallel_size=$PP_SIZE \\\n",
    "        \"\n",
    "\n",
    "\n",
    "export CMD=\" \\\n",
    "            python /dli/code/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "            --config-path=/dli/code/NeMo/examples/nlp/language_modeling/conf/ \\\n",
    "            --config-name=megatron_gpt_config.yaml \\\n",
    "            $TRAINER_ARGS \\\n",
    "            $PARALLEL_ARGS \\\n",
    "            $GPT_ARGS \\\n",
    "            $OUTPUT_ARGS \\\n",
    "            model.data.data_prefix=$DATA_PATH \\\n",
    "            model.data.data_impl=mmap \\\n",
    "            model.data.splits_string=\\\"949,50,1\\\" \\\n",
    "        \"\n",
    "\n",
    "bash -c '$LAUNCHER $CMD' 2>&1 | tee -a $LOGS_PATH/$NAME.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's run this script in an interactive session. To do so, follow the 3 steps:\n",
    "1. Launch a terminal session\n",
    "2. Run an interactive session by executing `srun -N 1 --pty /bin/bash`\n",
    "3. Run the NeMo gpt3 pretraining on 2 GPUs by executing `bash ./code/pretrain_gpt_2GPU.sh`\n",
    "\n",
    "Run the following cell to get the link to open a terminal session and the instructions to run an interactive session. Then, submit a pretraining job on 2 GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>\n",
       "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
       "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
       "   Step 3: Run the NeMo gpt3 pretraining on 2 GPUs: <font color=\"green\">bash ./code/pretrain_gpt_2GPU.sh</font>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following the <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 3: Run the NeMo gpt3 pretraining on 2 GPUs: <font color=\"green\">bash ./code/pretrain_gpt_2GPU.sh</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the GPT pretraining on 1 Node and 2 GPUs is running, we can check the SLURM queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                11  slurmpar     bash     root  R       0:19      1 slurmnode1\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also Check the GPUs using the `nvidia-smi` command. We should see GPU 0 and 1 utilized as shown in the figure bellow.\n",
    "\n",
    "<img src=\"images/1N_2gpus_utilization.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 21:29:33 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              74W / 300W |  46169MiB / 81920MiB |      4%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   45C    P0             328W / 300W |  48217MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              53W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU utilization on the master node\n",
    "!sleep 60s\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Understanding Multi-GPU NeMo Execution Logs\n",
    "\n",
    "Let's have a look at the execution logs:\n",
    "\n",
    "<img src=\"images/interactive_launch2.png\" width=\"900\"/>\n",
    "\n",
    "We can see that two processes with GLOBAL_RANK 0 and 1 were initialized. As we have 2 GPUs available, by default, the distributed strategy executed is the data parallel strategy, meaning that the model is copied on both GPUs and will process different data batches. \n",
    "\n",
    "To understand the performance of the GPT pretraining on 2 GPUs, we can check the generated [log file](./nemo/logs/log_2GPU.txt) during the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  59%|██████████████████▍            | 89/150 [01:26<00:59,  1.03it/s, loss=9.55, v_num=, reduced_train_loss=9.280, global_step=59.00, consumed_samples=1888.0, val_loss=9.800]\u001b[A\n",
      "Epoch 0:  60%|██████████████████▌            | 90/150 [01:26<00:57,  1.04it/s, loss=9.55, v_num=, reduced_train_loss=9.280, global_step=59.00, consumed_samples=1888.0, val_loss=9.220]\u001b[A\n",
      "                                                                                                                                                                                       \u001b[AEpoch 0, global step 60: 'val_loss' reached 9.21666 (best 9.21666), saving model to '/dli/nemo/1Node2GPUS/checkpoints/megatron_gpt--val_loss=9.22-step=60-consumed_samples=1888.0.ckpt' as top 10\n"
     ]
    }
   ],
   "source": [
    "!cat /dli/nemo/logs/1Node2GPUS.txt | tail -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the extract logs, notice the training performance while using 2 GPUs compared to 1 GPU.\n",
    "\n",
    "`Epoch 0: 100%|█| 150/150 [02:12<00:00,  1.13it/s, loss=8.48, v_num=, reduced_train_loss=8.50`\n",
    " \n",
    "Notice the number of samples consumed, and the number of iterations per second. With a batch size of 32 1.13 it/s is equivalent to 36.16 samples/s. Notice also that this is an almost linear increase which is a desirable property in multi-GPU systems.  \n",
    "\n",
    "Discuss the performance with the instructor. The major change here is larger number of samples processed in the same time duration, therefore helping the model learn richer data representations, speeding up the training.\n",
    "\n",
    "Great, before moving on, let's release some disk space by deleting the unnecessary checkpoints generated by the previous execution and cancel the remaining interactive session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the checkpoints\n",
    "!rm -rf /dli/nemo/1Node2GPUS/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3 Model Distribution Considerations \n",
    "\n",
    "To execute the previous Multi-GPU script in Tensor or Pipeline parallel mode, we can configure the distribution using the argument `model.tensor_model_parallel_size` or `model.pipeline_model_parallel_size`. \n",
    "\n",
    "The world size of NeMo GPT training corresponding to the number of GPUs will remain the same while the data_parallel_size, tensor_model_parallel_size and pipeline_model_parallel_size should be adjusted according to your configuration.\n",
    "\n",
    "The world size is the product of data_parallel_size, tensor_model_parallel_size and pipeline_model_parallel_size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "Great job with pretraining GPT-3 on a GPU cluster.<br>\n",
    "\n",
    "Before moving on, we need to make sure that no jobs are still running or waiting on the SLURM queue. \n",
    "Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                11  slurmpar     bash     root  R       2:32      1 slurmnode1\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                11  slurmpar     bash     root CG       2:35      1 slurmnode1\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be running  GPT language model training on multi-nodes distribution configurations. Move on to [03_GPT_LM_pretrainings_multinodes.ipynb](03_GPT_LM_pretrainings_multinodes.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
